{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0]\n",
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import stim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "distance=3\n",
    "num_ancilla_qubits=8\n",
    "rounds=5\n",
    "\n",
    "path = r\"C:\\Users\\conof\\Desktop\\RNN_QEC\\google_qec3v5_experiment_data\\surface_code_bX_d3_r05_center_3_5\\circuit_noisy.stim\"\n",
    "circuit_google = stim.Circuit.from_file(path)\n",
    "\n",
    "surface_code_circuit = stim.Circuit.generated(\n",
    "    \"surface_code:rotated_memory_x\",\n",
    "    rounds=5,\n",
    "    distance=3,\n",
    "    after_clifford_depolarization=0.01,\n",
    "    after_reset_flip_probability=0.01,\n",
    "    before_measure_flip_probability=0.01,\n",
    "    before_round_data_depolarization=0.01)\n",
    "\n",
    "num_shots=5000*64\n",
    "# Compile the sampler\n",
    "sampler = circuit_google.compile_detector_sampler()\n",
    "# Sample shots, with observables\n",
    "detection_events, observable_flips = sampler.sample(num_shots, separate_observables=True)\n",
    "\n",
    "\n",
    "detection_events = detection_events.astype(int)\n",
    "detection_strings = [''.join(map(str, row)) for row in detection_events] #compress the detection events in a tensor\n",
    "detection_events_numeric = [[int(value) for value in row] for row in detection_events] # Convert string elements to integers (or floats if needed)\n",
    "detection_array = np.array(detection_events_numeric) # Convert detection_events to a numpy array\n",
    "print(detection_array[0])\n",
    "\n",
    "detection_array1 = detection_array.reshape(num_shots, rounds, num_ancilla_qubits) #first dim is the number of shots, second dim round number, third dim is the Ancilla \n",
    "print(detection_array1[0]) \n",
    "\n",
    "observable_flips = observable_flips.astype(int).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_size, layers_sizes, hidden_size):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        \n",
    "        # Define the input layer\n",
    "        self.input_layer = nn.Linear(input_size, layers_sizes[0])\n",
    "        \n",
    "        # Define hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(layers_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(layers_sizes[i], layers_sizes[i + 1]))\n",
    "        \n",
    "        # Define output layer\n",
    "        self.output_layer = nn.Linear(layers_sizes[-1], hidden_size)\n",
    "\n",
    "        # Define activation function (e.g., ReLU)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through input layer\n",
    "        #torch.transpose(x.squeeze(0), 1,0)\n",
    "\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        \n",
    "        # Pass through hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        # Pass through output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatticeRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,layers_sizes,batch_size):\n",
    "        super(LatticeRNNCell, self).__init__()\n",
    "        self.hidden=hidden_size\n",
    "        self.batch_size=batch_size\n",
    "        self.fc_input = nn.Linear(input_size, input_size)\n",
    "        #self.fc_hidden_double = nn.Linear(hidden_size*3, hidden_size)\n",
    "        self.multi_layer_fc = FullyConnectedNN(hidden_size*3, layers_sizes, hidden_size)\n",
    "        self.multi_layer_fc1 = FullyConnectedNN(hidden_size*3, layers_sizes, hidden_size)\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden_left, hidden_up, hidden_ext, hidden_ext1):\n",
    "        # if hidden_left is not None and hidden_bottom is not None:\n",
    "        #     hidden = (hidden_left + hidden_bottom) / 2  # Average of left and bottom\n",
    "        # elif hidden_left is not None:\n",
    "        #     hidden = hidden_left\n",
    "        # elif hidden_bottom is not None:\n",
    "        #     hidden = hidden_bottom\n",
    "        # else:\n",
    "        #     hidden = torch.zeros(x.size(0), self.hidden_size).to(x.device)  # Initial hidden state\n",
    "        \n",
    "        #input fc net\n",
    "        #input=self.fc_input(x.type(torch.FloatTensor))\n",
    "\n",
    "        \"\"\"\n",
    "        # Combine the hidden states from left and bottom\n",
    "        if hidden_left is not None and hidden_up is not None:\n",
    "            combined_hidden=torch.cat((hidden_left,hidden_up),1)\n",
    "            hidden=self.fc_hidden_double(combined_hidden.squeeze(0)).unsqueeze(0)\n",
    "\n",
    "        elif hidden_left is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_left.squeeze(0)).unsqueeze(0)\n",
    "\n",
    "        elif hidden_up is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_up.squeeze(0)).unsqueeze(0)\n",
    "        else:\n",
    "            hidden=torch.zeros(1,self.hidden, dtype=torch.float)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Combine the hidden states from left and bottom\n",
    "        if hidden_left[0] is not None and hidden_up[0] is not None:\n",
    "            combined_hidden=torch.cat((hidden_left[0],hidden_up[0], hidden_ext),1)\n",
    "        elif hidden_left[0] is not None:\n",
    "            hidden_init_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden=torch.cat((hidden_left[0],hidden_init_up, hidden_ext),1)\n",
    "        elif hidden_up[0] is not None:\n",
    "            hidden_init_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden=torch.cat((hidden_init_left,hidden_up[0], hidden_ext),1)\n",
    "        else:\n",
    "            hidden_init_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            hidden_init_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden=torch.cat((hidden_init_left,hidden_init_up, hidden_ext),1)\n",
    "\n",
    "\n",
    "\n",
    "        if hidden_left[1] is not None and hidden_up[1] is not None:\n",
    "            combined_hidden1=torch.cat((hidden_left[1],hidden_up[1], hidden_ext1),1)\n",
    "        elif hidden_left[1] is not None:\n",
    "            hidden_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden1=torch.cat((hidden_left[1],hidden_up, hidden_ext1),1)\n",
    "        elif hidden_up[1] is not None:\n",
    "            hidden_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden1=torch.cat((hidden_left,hidden_up[1], hidden_ext1),1)\n",
    "        else:\n",
    "            hidden_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            hidden_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden1=torch.cat((hidden_left,hidden_up, hidden_ext1),1)\n",
    "\n",
    "\n",
    "        hidden=self.multi_layer_fc(combined_hidden)\n",
    "        hidden1=self.multi_layer_fc1(combined_hidden1)\n",
    "\n",
    "        x=x.squeeze(1).float()\n",
    "\n",
    "        # Update hidden state using current input and combined hidden state\n",
    "        hidden,hidden1 = self.lstm_cell(x, (hidden,hidden1))\n",
    "        \n",
    "        return hidden,hidden1\n",
    "\n",
    "class LatticeRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width,layers_sizes, batch_size):\n",
    "        super(LatticeRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.grid_height = grid_height\n",
    "        self.grid_width = grid_width \n",
    "        self.lstm_cells = nn.ModuleList([LatticeRNNCell(input_size, hidden_size, layers_sizes, batch_size) for _ in range(grid_height * grid_width)])\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, hidden_ext, hidden_ext1):\n",
    "        # Initialize a grid of hidden states\n",
    "        grid = [[None for _ in range(self.grid_width)] for _ in range(self.grid_height)]\n",
    "        \n",
    "        # Reshape the input to match the grid size\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.reshape(batch_size,self.grid_height, self.grid_width)\n",
    "\n",
    "        for i in range(self.grid_height):\n",
    "            for j in range(self.grid_width):\n",
    "                input_bit = x[:,i,j].unsqueeze(1).unsqueeze(1)  # Get the input for the current cell\n",
    "                if j > 0:\n",
    "                    hidden_left = grid[i][j - 1]\n",
    "                else:\n",
    "                    hidden_left = (None,None)\n",
    "\n",
    "                if i > 0:\n",
    "                    hidden_up = grid[i-1][j]\n",
    "                else:\n",
    "                    hidden_up = (None,None)\n",
    "\n",
    "                # Get the index for the current RNN cell\n",
    "                cell_index = i * self.grid_width + j\n",
    "                hidden, hidden1 = self.lstm_cells[cell_index](input_bit, hidden_left, hidden_up, hidden_ext, hidden_ext1)\n",
    "\n",
    "                # Store the hidden state in the grid\n",
    "                grid[i][j] = (hidden,hidden1)\n",
    "\n",
    "        # The output that matters is the hidden state from the top-right corner (i.e., grid[grid_size-1][grid_size-1])\n",
    "        bottom_right_hidden,  bottom_right_hidden1 = grid[-1][-1]\n",
    "\n",
    "        # Pass the hidden state through the fully connected layer and sigmoid for binary output\n",
    "        #hidden= self.fc_hidden(bottom_right_hidden)\n",
    "        output = self.fc_out(bottom_right_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output,  bottom_right_hidden, bottom_right_hidden1\n",
    "    \n",
    "\n",
    "\n",
    "# RNN model\n",
    "class BlockRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width, rounds,layers_sizes,batch_size):\n",
    "        super(BlockRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.fc_in = nn.Linear(input_size, input_size)\n",
    "        self.rnn_block = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width, layers_sizes,batch_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, rounds):\n",
    "        #input=self.fc_in(x)\n",
    "\n",
    "        hidden_ext = torch.zeros(self.batch_size,self.hidden_size) # (1,hidden_size) hidden state of the previous round \n",
    "        hidden_ext1 = torch.zeros(self.batch_size,self.hidden_size)\n",
    "\n",
    "        for round in range (rounds):\n",
    "            input_block = x[:,round,:].unsqueeze(2)  # (1, 8) syndromes\n",
    "            out, hidden_ext, hidden_ext1 = self.rnn_block(input_block, hidden_ext, hidden_ext1)\n",
    "\n",
    "        #I already use fc, sigmoid in the LatticeRNN\n",
    "        #out = self.fc_out(out)  # Use the last time-step's output, needed for changing the dimension of the output compared of input\n",
    "        #out = self.sigmoid(out)  # I need a Binary output\n",
    "        return out, hidden_ext[0]\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs, batch_size,rounds):\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    # If batch_size is None, set it to the full size of the dataset (no mini-batching)\n",
    "\n",
    "    num_samples = len(X_train[:,0,0])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # # Shuffle the dataset at the beginning of each epoch\n",
    "        # permutation = torch.randperm(num_samples)\n",
    "        # X_train = X_train[permutation]\n",
    "        # y_train = y_train[permutation]\n",
    "\n",
    "        # Mini-batch training loop\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            # Create mini-batches\n",
    "            batch_x = torch.from_numpy(X_train[i:i + batch_size])\n",
    "            batch_y = torch.Tensor(y_train[i:i + batch_size])\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            #hidden = model.init_hidden(1)\n",
    "            output, hidden= model(batch_x, rounds)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.squeeze(1), batch_y)\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize (update weights)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for logging purposes\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print average loss after each epoch\n",
    "        avg_loss = running_loss / (num_samples // batch_size)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_array_to_tensor(binary_array):\n",
    "    # Check if the input is a NumPy array, if not, convert it\n",
    "    if isinstance(binary_array, np.ndarray):\n",
    "        tensor = torch.from_numpy(binary_array).float()  # Convert NumPy array to float32 tensor\n",
    "    else:\n",
    "        # If not a NumPy array, convert it as before\n",
    "        tensor = torch.tensor([[int(bit) for bit in binary_array]], dtype=torch.float32)\n",
    "    return tensor.unsqueeze(0)  # Add batch dimension (batch_size = 1)\n",
    "\n",
    "\n",
    "def test(model, test_sequences, targets,batch_size):\n",
    "    model.eval()  # Set the model to evaluation mode (disable dropout, etc.)\n",
    "    correct = 0\n",
    "\n",
    "    hidden = None\n",
    "    num_samples = len(test_sequences[:,0,0])\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            \n",
    "            output=np.zeros(batch_size)\n",
    "            batch_x = torch.from_numpy(test_sequences[i:i + batch_size])\n",
    "            target = targets[i:i + batch_size]\n",
    "            rounds = len(batch_x[0,:,0])\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            #if hidden is None:\n",
    "                # Initialize hidden state for the first sequence (or batch)\n",
    "            #    hidden = model.init_hidden(batch_size=1)  # batch_size might vary depending on your use case\n",
    "            #else:\n",
    "                #hidden=hidden.detach() #you detach if you want to avoid that the gradient is propagated through the hidden states to avoid long training time and memory usage\n",
    "            \n",
    "            # Forward pass for prediction\n",
    "            output, hidden = model(batch_x, rounds)\n",
    "            prediction = torch.round(output)  # Convert probability to binary (0 or 1)\n",
    "            \n",
    "            for j in range(0,batch_size):\n",
    "                if prediction[j] == target[j]:\n",
    "                    correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_sequences)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0142,  0.1184, -0.0046,  0.1082,  0.1224,  0.0833, -0.0281, -0.0878],\n",
      "        [ 0.1254,  0.1305, -0.0388,  0.1722,  0.0897, -0.0030, -0.1370,  0.0161],\n",
      "        [ 0.1388,  0.1341, -0.0499,  0.1764,  0.0715, -0.0210, -0.1598,  0.0434],\n",
      "        [ 0.0893,  0.1245, -0.0217,  0.1407,  0.1061,  0.0277, -0.0899, -0.0296],\n",
      "        [ 0.1020,  0.1241, -0.0225,  0.1550,  0.1023,  0.0219, -0.1084, -0.0091],\n",
      "        [-0.0341,  0.1096,  0.0033,  0.0809,  0.1285,  0.1077,  0.0190, -0.1046],\n",
      "        [ 0.0853,  0.1190, -0.0151,  0.1455,  0.1088,  0.0336, -0.0897, -0.0258],\n",
      "        [ 0.1150,  0.1262, -0.0259,  0.1654,  0.0959,  0.0076, -0.1232,  0.0124],\n",
      "        [ 0.0575,  0.1184, -0.0076,  0.1295,  0.1195,  0.0520, -0.0623, -0.0498],\n",
      "        [ 0.1419,  0.1325, -0.0468,  0.1769,  0.0642, -0.0292, -0.1556,  0.0762],\n",
      "        [ 0.0175,  0.1187, -0.0061,  0.1078,  0.1215,  0.0809, -0.0316, -0.0862],\n",
      "        [ 0.1383,  0.1325, -0.0457,  0.1731,  0.0686, -0.0204, -0.1510,  0.0517],\n",
      "        [ 0.1254,  0.1321, -0.0356,  0.1747,  0.0897, -0.0005, -0.1420,  0.0240],\n",
      "        [ 0.0829,  0.1182, -0.0134,  0.1405,  0.1090,  0.0319, -0.0850, -0.0253],\n",
      "        [-0.0144,  0.1136,  0.0012,  0.0920,  0.1286,  0.0972,  0.0036, -0.0956],\n",
      "        [ 0.1391,  0.1283, -0.0400,  0.1846,  0.0797, -0.0223, -0.1517,  0.0715],\n",
      "        [ 0.0343,  0.1145, -0.0030,  0.1133,  0.1193,  0.0703, -0.0353, -0.0625],\n",
      "        [ 0.1475,  0.1353, -0.0552,  0.1848,  0.0603, -0.0387, -0.1652,  0.0998],\n",
      "        [ 0.1484,  0.1320, -0.0559,  0.1802,  0.0582, -0.0364, -0.1620,  0.0950],\n",
      "        [ 0.1351,  0.1247, -0.0377,  0.1724,  0.0786, -0.0191, -0.1429,  0.0569],\n",
      "        [ 0.0966,  0.1160, -0.0169,  0.1425,  0.1016,  0.0171, -0.0940, -0.0074],\n",
      "        [ 0.0942,  0.1240, -0.0193,  0.1487,  0.1060,  0.0242, -0.1008, -0.0179],\n",
      "        [ 0.0345,  0.1227, -0.0134,  0.1133,  0.1180,  0.0696, -0.0460, -0.0811],\n",
      "        [ 0.0342,  0.1183, -0.0065,  0.1088,  0.1178,  0.0619, -0.0347, -0.0694],\n",
      "        [-0.0246,  0.1134,  0.0015,  0.0844,  0.1263,  0.1011,  0.0141, -0.1009],\n",
      "        [ 0.0347,  0.1218, -0.0126,  0.1186,  0.1188,  0.0707, -0.0534, -0.0842],\n",
      "        [ 0.0526,  0.1191, -0.0073,  0.1268,  0.1199,  0.0546, -0.0579, -0.0554],\n",
      "        [ 0.0837,  0.1170, -0.0125,  0.1424,  0.1100,  0.0326, -0.0851, -0.0226],\n",
      "        [ 0.0526,  0.1196, -0.0088,  0.1280,  0.1205,  0.0558, -0.0621, -0.0588],\n",
      "        [ 0.0931,  0.1268, -0.0240,  0.1515,  0.0995,  0.0252, -0.1052, -0.0295],\n",
      "        [ 0.1380,  0.1246, -0.0384,  0.1750,  0.0685, -0.0257, -0.1503,  0.0783],\n",
      "        [ 0.1424,  0.1390, -0.0528,  0.1846,  0.0646, -0.0271, -0.1660,  0.0644],\n",
      "        [ 0.0988,  0.1241, -0.0214,  0.1485,  0.1028,  0.0197, -0.1023, -0.0124],\n",
      "        [ 0.0304,  0.1181, -0.0065,  0.1192,  0.1218,  0.0746, -0.0451, -0.0778],\n",
      "        [ 0.0578,  0.1174, -0.0111,  0.1311,  0.1132,  0.0487, -0.0690, -0.0571],\n",
      "        [ 0.1352,  0.1272, -0.0405,  0.1759,  0.0750, -0.0198, -0.1500,  0.0461],\n",
      "        [ 0.1542,  0.1373, -0.0758,  0.1897,  0.0257, -0.0576, -0.1817,  0.1490],\n",
      "        [ 0.0916,  0.1266, -0.0205,  0.1462,  0.1065,  0.0275, -0.0970, -0.0241],\n",
      "        [ 0.1134,  0.1244, -0.0284,  0.1611,  0.0916,  0.0068, -0.1281,  0.0023],\n",
      "        [ 0.0669,  0.1217, -0.0148,  0.1315,  0.1133,  0.0463, -0.0714, -0.0501],\n",
      "        [ 0.1099,  0.1226, -0.0225,  0.1556,  0.0964,  0.0027, -0.1140,  0.0091],\n",
      "        [ 0.1506,  0.1370, -0.0634,  0.1831,  0.0465, -0.0413, -0.1731,  0.0968],\n",
      "        [ 0.1543,  0.1431, -0.0742,  0.1829,  0.0346, -0.0528, -0.1785,  0.1195],\n",
      "        [ 0.0711,  0.1257, -0.0177,  0.1362,  0.1117,  0.0429, -0.0793, -0.0520],\n",
      "        [ 0.1280,  0.1283, -0.0352,  0.1683,  0.0831, -0.0107, -0.1393,  0.0361],\n",
      "        [ 0.0501,  0.1221, -0.0084,  0.1258,  0.1228,  0.0548, -0.0560, -0.0600],\n",
      "        [ 0.1149,  0.1264, -0.0286,  0.1653,  0.0984,  0.0091, -0.1248,  0.0062],\n",
      "        [ 0.0966,  0.1219, -0.0199,  0.1485,  0.1012,  0.0234, -0.1018, -0.0143],\n",
      "        [ 0.1530,  0.1309, -0.0765,  0.1829,  0.0221, -0.0638, -0.1820,  0.1613],\n",
      "        [ 0.0191,  0.1163, -0.0046,  0.1111,  0.1227,  0.0798, -0.0349, -0.0848],\n",
      "        [ 0.0062,  0.1122, -0.0005,  0.1033,  0.1231,  0.0923, -0.0188, -0.0833],\n",
      "        [ 0.1008,  0.1226, -0.0204,  0.1546,  0.1059,  0.0172, -0.1086, -0.0075],\n",
      "        [ 0.1328,  0.1278, -0.0399,  0.1712,  0.0782, -0.0128, -0.1450,  0.0408],\n",
      "        [ 0.1524,  0.1436, -0.0817,  0.1919,  0.0255, -0.0589, -0.1850,  0.1581],\n",
      "        [ 0.1307,  0.1344, -0.0393,  0.1731,  0.0783, -0.0105, -0.1450,  0.0377],\n",
      "        [ 0.1416,  0.1384, -0.0520,  0.1807,  0.0666, -0.0251, -0.1623,  0.0564],\n",
      "        [ 0.1310,  0.1360, -0.0480,  0.1711,  0.0811, -0.0120, -0.1492,  0.0111],\n",
      "        [ 0.1387,  0.1318, -0.0463,  0.1740,  0.0710, -0.0274, -0.1547,  0.0532],\n",
      "        [ 0.1457,  0.1290, -0.0479,  0.1820,  0.0620, -0.0337, -0.1584,  0.0938],\n",
      "        [ 0.0600,  0.1152, -0.0129,  0.1277,  0.1136,  0.0505, -0.0690, -0.0566],\n",
      "        [-0.0188,  0.1117,  0.0004,  0.0891,  0.1228,  0.1033,  0.0030, -0.1007],\n",
      "        [ 0.1523,  0.1377, -0.0678,  0.1890,  0.0388, -0.0497, -0.1806,  0.0917],\n",
      "        [ 0.1335,  0.1269, -0.0362,  0.1740,  0.0798, -0.0178, -0.1437,  0.0583],\n",
      "        [ 0.0258,  0.1167, -0.0048,  0.1159,  0.1253,  0.0775, -0.0375, -0.0772]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 1  # Each RNN cell takes 1 bit as input\n",
    "hidden_size = 8 # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output\n",
    "grid_height = 4  # 4x2 grid of RNN cells\n",
    "grid_width= 2\n",
    "batch_size=64\n",
    "layers_sizes=[hidden_size*3,hidden_size*2,hidden_size ]\n",
    "\n",
    "model = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width, layers_sizes,batch_size)\n",
    "\n",
    "# Input is a batch of binary sequences (batch_size, seq_len, input_size), reshaped to match grid size\n",
    "seq_len = grid_height * grid_width  # Number of bits equal to the grid size squared\n",
    "x = torch.randn(batch_size, seq_len, input_size)  # Random input, replace with binary input\n",
    "hidden_ext = torch.randn(batch_size, hidden_size)  # Random input, replace with binary input\n",
    "hidden_ext1 = torch.randn(batch_size, hidden_size)  # Random input, replace with binary input\n",
    "\n",
    "output, hidden, hidden1 = model(x,hidden_ext,hidden_ext1)\n",
    "print(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6251\n",
      "Epoch [2/10], Loss: 0.6061\n",
      "Epoch [3/10], Loss: 0.5776\n",
      "Epoch [4/10], Loss: 0.6120\n",
      "Epoch [5/10], Loss: 0.5989\n",
      "Epoch [6/10], Loss: 0.5791\n",
      "Epoch [7/10], Loss: 0.5642\n",
      "Epoch [8/10], Loss: 0.5336\n",
      "Epoch [9/10], Loss: 0.5254\n",
      "Epoch [10/10], Loss: 0.5185\n",
      "Training finished.\n",
      "Test Accuracy: 72.67%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 1  # Each Lattice RNN cell takes 1 bit as input\n",
    "hidden_size = 64 # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output (e.g., 0 or 1)\n",
    "grid_height = 4  # Number of rows in the grid\n",
    "grid_width = 2   # Number of columns in the grid\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "layers_sizes=[hidden_size*3,hidden_size*2,hidden_size ]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a model instance\n",
    "model = BlockRNN(input_size, hidden_size, output_size, grid_height, grid_width, rounds,layers_sizes,batch_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "test_size=0.2\n",
    "test_dataset_size=num_shots*test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(detection_array1, observable_flips, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs,batch_size,rounds)\n",
    "\n",
    "test(model, X_test, y_test,batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
