{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import perf_counter\n",
    "\n",
    "distance=3\n",
    "num_ancilla_qubits=8\n",
    "rounds=5\n",
    "\n",
    "surface_code_circuit = stim.Circuit.generated(\n",
    "    \"surface_code:rotated_memory_x\",\n",
    "    rounds=5,\n",
    "    distance=3,\n",
    "    after_clifford_depolarization=0.01,\n",
    "    after_reset_flip_probability=0.01,\n",
    "    before_measure_flip_probability=0.01,\n",
    "    before_round_data_depolarization=0.01)\n",
    "\n",
    "\n",
    "num_shots=2000000\n",
    "# Compile the sampler\n",
    "sampler = surface_code_circuit.compile_detector_sampler()\n",
    "# Sample shots, with observables\n",
    "detection_events, observable_flips = sampler.sample(num_shots, separate_observables=True)\n",
    "\n",
    "\n",
    "detection_events = detection_events.astype(int)\n",
    "detection_strings = [''.join(map(str, row)) for row in detection_events] #compress the detection events in a tensor\n",
    "detection_events_numeric = [[int(value) for value in row] for row in detection_events] # Convert string elements to integers (or floats if needed)\n",
    "detection_array = np.array(detection_events_numeric) # Convert detection_events to a numpy array\n",
    "print(detection_array[0])\n",
    "\n",
    "detection_array1 = detection_array.reshape(num_shots, rounds, num_ancilla_qubits) #first dim is the number of shots, second dim round number, third dim is the Ancilla \n",
    "print(detection_array1[0]) \n",
    "\n",
    "observable_flips = observable_flips.astype(int).flatten().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class LatticeRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,batch_size):\n",
    "        super(LatticeRNNCell, self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.hidden=hidden_size\n",
    "        self.fc_input = nn.Linear(input_size, input_size)\n",
    "        self.fc_hidden_double = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_hidden_single = nn.Linear(hidden_size, hidden_size)\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden_left, hidden_up):\n",
    "        # if hidden_left is not None and hidden_bottom is not None:\n",
    "        #     hidden = (hidden_left + hidden_bottom) / 2  # Average of left and bottom\n",
    "        # elif hidden_left is not None:\n",
    "        #     hidden = hidden_left\n",
    "        # elif hidden_bottom is not None:\n",
    "        #     hidden = hidden_bottom\n",
    "        # else:\n",
    "        #     hidden = torch.zeros(x.size(0), self.hidden_size).to(x.device)  # Initial hidden state\n",
    "        \n",
    "        #input fc net\n",
    "        #input=self.fc_input(x.type(torch.FloatTensor))\n",
    "\n",
    "        # Combine the hidden states from left and bottom\n",
    "        if hidden_left is not None and hidden_up is not None:\n",
    "            combined_hidden=torch.cat((hidden_left,hidden_up),1)\n",
    "            hidden=self.fc_hidden_double(combined_hidden.squeeze(0))\n",
    "\n",
    "        elif hidden_left is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_left)\n",
    "\n",
    "        elif hidden_up is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_up)\n",
    "            \n",
    "        else:\n",
    "            hidden=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "\n",
    "        x=x.squeeze(1).float()\n",
    "\n",
    "        # Update hidden state using current input and combined hidden state\n",
    "        hidden = self.rnn_cell(x, hidden)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "class LatticeRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width,batch_size):\n",
    "        super(LatticeRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.grid_height = grid_height\n",
    "        self.grid_width = grid_width \n",
    "        self.rnn_cells = nn.ModuleList([LatticeRNNCell(input_size, hidden_size,batch_size) for _ in range(grid_height * grid_width)])\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, hidden_ext):\n",
    "        # Initialize a grid of hidden states\n",
    "        grid = [[None for _ in range(self.grid_width)] for _ in range(self.grid_height)]\n",
    "        \n",
    "        # Reshape the input to match the grid size\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.reshape(batch_size, self.grid_height, self.grid_width)\n",
    "\n",
    "        for i in range(self.grid_height):\n",
    "            for j in range(self.grid_width):\n",
    "                input_bit = x[:,i,j].unsqueeze(1).unsqueeze(1) # Get the input for the current cell\n",
    "                if j==0 & i==0:\n",
    "                    hidden_left = hidden_ext\n",
    "                hidden_left = grid[i][j - 1] if j > 0 else None\n",
    "                hidden_up = grid[i - 1][j] if i > 0 else None\n",
    "\n",
    "                # Get the index for the current RNN cell\n",
    "                cell_index = i * self.grid_width + j\n",
    "                hidden = self.rnn_cells[cell_index](input_bit, hidden_left, hidden_up)\n",
    "\n",
    "                # Store the hidden state in the grid\n",
    "                grid[i][j] = hidden\n",
    "\n",
    "        # The output that matters is the hidden state from the top-right corner (i.e., grid[grid_size-1][grid_size-1])\n",
    "        bottom_right_hidden = grid[-1][-1]\n",
    "\n",
    "        # Pass the hidden state through the fully connected layer and sigmoid for binary output\n",
    "        hidden= self.fc_hidden(bottom_right_hidden)\n",
    "        output = self.fc_out(bottom_right_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "\n",
    "# RNN model\n",
    "class BlockRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width, rounds,batch_size):\n",
    "        super(BlockRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.fc_in = nn.Linear(input_size, input_size)\n",
    "        self.rnn_block = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width,batch_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, rounds):\n",
    "        #input=self.fc_in(x)\n",
    "\n",
    "        hidden_ext = torch.zeros(1,1,self.hidden_size)\n",
    "\n",
    "        for round in range (rounds):\n",
    "            input_block = x[:,round,:].unsqueeze(2)  # (1, 8)\n",
    "            out, hidden_ext = self.rnn_block(input_block, hidden_ext)\n",
    "\n",
    "        #I already use fc, sigmoid in the LatticeRNN\n",
    "        #out = self.fc_out(out)  # Use the last time-step's output, needed for changing the dimension of the output compared of input\n",
    "        #out = self.sigmoid(out)  # I need a Binary output\n",
    "        return out, hidden_ext\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs, batch_size,rounds):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    num_batches = len(X_train[:,0,0]) // batch_size\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            # Create mini-batches\n",
    "            batch_x = torch.from_numpy(X_train[batch_idx * batch_size : (batch_idx + 1) * batch_size])\n",
    "            batch_y = torch.Tensor(y_train[batch_idx * batch_size : (batch_idx + 1) * batch_size])\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            #hidden = model.init_hidden(1)\n",
    "            output, hidden = model(batch_x, rounds)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.squeeze(1), batch_y)\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize (update weights)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for logging purposes\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print average loss after each epoch\n",
    "        avg_loss = running_loss / num_batches \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "\n",
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_batch(batch_x, batch_y,i, model, criterion, rounds):\n",
    "    # Forward pass\n",
    "    outputs, hidden = model(batch_x, rounds)\n",
    "    loss = criterion(outputs.squeeze(1), batch_y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Collect gradients\n",
    "    grads = [param.grad.clone() if param.grad is not None else torch.zeros_like(param)\n",
    "             for param in model.parameters()]\n",
    "    \n",
    "    print(i)\n",
    "    print(\"ff\")\n",
    "    \n",
    "    # Reduce gradients over batch dimension\n",
    "    #reduced_grads = [grad.sum(dim=0, keepdim=True) if len(grad.shape) > 1 else grad \n",
    "    #               for grad in grads]\n",
    "    \n",
    "    return loss.item(), grads\n",
    "\n",
    "\n",
    "def train_rnn_parallel(model,  X_train, y_train, criterion, optimizer,  num_epochs, batch_size, rounds, n_jobs=8):\n",
    "    \n",
    "    num_samples = len(X_train[:,0,0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0  # Reset loss for each epoch\n",
    "\n",
    "        # Ensure model is in training mode\n",
    "        model.train()\n",
    "\n",
    "        # Split data into batches\n",
    "        batches = [\n",
    "            (torch.from_numpy(X_train[i:i + batch_size]),\n",
    "            torch.Tensor(y_train[i:i + batch_size]))\n",
    "            for i in range(0, num_samples, batch_size)\n",
    "        ]\n",
    "\n",
    "        start= perf_counter()\n",
    "        #results = [process_batch(batch_x, batch_y,i, model, criterion, rounds) for i, (batch_x, batch_y) in enumerate(batches)]\n",
    "        results = Parallel(n_jobs=n_jobs,  backend='multiprocessing')(delayed(process_batch)(batch_x, batch_y,i,  model, criterion, rounds)for i, (batch_x, batch_y) in enumerate(batches))   \n",
    "        end_time = perf_counter()\n",
    "\n",
    "        elapsed_time = end_time - start\n",
    "        print(f\"Execution time: {elapsed_time:.6f} seconds\")\n",
    "        \n",
    "        # Aggregate results\n",
    "        optimizer.zero_grad()  # Clear gradients before aggregation\n",
    "        \n",
    "        for loss, grads in results:\n",
    "            running_loss += loss\n",
    "            for param, grad in zip(model.parameters(), grads):\n",
    "                if grad.shape != param.shape:\n",
    "                    raise ValueError(f\"Gradient shape {grad.shape} does not match parameter shape {param.shape}.\")\n",
    "                if param.grad is None:\n",
    "                    param.grad = grad.clone()  # Initialize gradient\n",
    "                else:\n",
    "                    param.grad += grad  # Accumulate gradient\n",
    "\n",
    "\n",
    "        # Step optimizer after aggregating all gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the epoch's loss\n",
    "        avg_loss = running_loss / len(batches)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_array_to_tensor(binary_array):\n",
    "    # Check if the input is a NumPy array, if not, convert it\n",
    "    if isinstance(binary_array, np.ndarray):\n",
    "        tensor = torch.from_numpy(binary_array).float()  # Convert NumPy array to float32 tensor\n",
    "    else:\n",
    "        # If not a NumPy array, convert it as before\n",
    "        tensor = torch.tensor([[int(bit) for bit in binary_array]], dtype=torch.float32)\n",
    "    return tensor.unsqueeze(0)  # Add batch dimension (batch_size = 1)\n",
    "\n",
    "\n",
    "def test(model, test_sequences, targets,batch_size):\n",
    "    model.eval()  # Set the model to evaluation mode (disable dropout, etc.)\n",
    "    correct = 0\n",
    "\n",
    "    hidden = None\n",
    "    num_batches = len(test_sequences[:,0,0]) // batch_size\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for batch_idx in range(num_batches):\n",
    "        \n",
    "            output=np.zeros(batch_size)\n",
    "            batch_x = torch.from_numpy(test_sequences[batch_idx * batch_size : (batch_idx + 1) * batch_size])\n",
    "            target = targets[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "            rounds = len(batch_x[0,:,0])\n",
    "            \n",
    "            \n",
    "            # Initialize hidden state\n",
    "            #if hidden is None:\n",
    "                # Initialize hidden state for the first sequence (or batch)\n",
    "            #    hidden = model.init_hidden(batch_size=1)  # batch_size might vary depending on your use case\n",
    "            #else:\n",
    "                #hidden=hidden.detach() #you detach if you want to avoid that the gradient is propagated through the hidden states to avoid long training time and memory usage\n",
    "            \n",
    "            # Forward pass for prediction\n",
    "            output, hidden = model(batch_x, rounds)\n",
    "            prediction = torch.round(output)  # Convert probability to binary (0 or 1)\n",
    "            \n",
    "            for j in range(0,batch_size):\n",
    "                if prediction[j] == target[j]:\n",
    "                    correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_sequences)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2389e-01,  5.2057e-02, -6.8535e-01,  9.1728e-02,  1.6810e-01,\n",
      "          5.7598e-01,  2.4971e-01, -4.3904e-01],\n",
      "        [ 4.3874e-02,  1.1276e-01, -6.1573e-01,  1.2015e-03,  2.4817e-01,\n",
      "          5.8337e-01,  3.5307e-01, -2.5986e-01],\n",
      "        [ 2.7548e-02,  1.2819e-01, -5.6650e-01,  3.2068e-01,  1.3603e-01,\n",
      "          3.6548e-01,  2.0271e-01, -6.4674e-01],\n",
      "        [ 7.9232e-02,  8.7935e-02, -6.3122e-01,  1.4696e-01,  1.8776e-01,\n",
      "          5.1412e-01,  2.7289e-01, -4.5864e-01],\n",
      "        [ 9.4474e-02,  8.4767e-02, -6.6991e-01, -7.0777e-02,  2.3558e-01,\n",
      "          6.6425e-01,  3.4709e-01, -2.0468e-01],\n",
      "        [ 2.4849e-03,  1.4484e-01, -5.2711e-01,  6.4403e-02,  3.2605e-01,\n",
      "          5.3024e-01,  3.9275e-01, -2.1342e-01],\n",
      "        [ 9.5882e-02,  6.2609e-02, -6.3396e-01,  1.6601e-01,  2.0550e-01,\n",
      "          5.1474e-01,  2.4832e-01, -4.7617e-01],\n",
      "        [ 5.4707e-02,  1.1258e-01, -6.0876e-01,  2.3590e-01,  1.4510e-01,\n",
      "          4.3759e-01,  2.3953e-01, -5.7213e-01],\n",
      "        [ 8.3717e-02,  9.3303e-02, -6.4895e-01,  2.3236e-02,  2.0519e-01,\n",
      "          5.9678e-01,  3.2902e-01, -3.0718e-01],\n",
      "        [ 7.2774e-02,  9.6052e-02, -6.1347e-01,  3.1604e-02,  2.7229e-01,\n",
      "          5.9837e-01,  3.4833e-01, -2.5195e-01],\n",
      "        [ 7.0979e-02,  1.0326e-01, -6.3249e-01, -1.2912e-01,  2.7513e-01,\n",
      "          6.8691e-01,  4.0932e-01, -7.5475e-02],\n",
      "        [ 6.7384e-02,  9.3040e-02, -6.1434e-01,  2.7057e-02,  2.7913e-01,\n",
      "          5.8928e-01,  3.4233e-01, -2.6740e-01],\n",
      "        [ 7.3822e-02,  7.6755e-02, -5.8929e-01,  3.6470e-01,  9.4153e-02,\n",
      "          3.7133e-01,  1.6389e-01, -6.9523e-01],\n",
      "        [ 8.1852e-02,  6.0724e-02, -6.1719e-01,  3.2198e-01,  1.0297e-01,\n",
      "          3.9322e-01,  1.6620e-01, -6.9690e-01],\n",
      "        [ 6.5260e-02,  8.6773e-02, -5.7763e-01,  3.4440e-01,  1.3109e-01,\n",
      "          3.8067e-01,  1.8220e-01, -6.5260e-01],\n",
      "        [ 6.3829e-02,  9.2950e-02, -6.3148e-01, -8.6707e-02,  2.7805e-01,\n",
      "          6.4927e-01,  3.7695e-01, -1.5209e-01],\n",
      "        [ 2.6267e-02,  1.2431e-01, -5.8758e-01,  1.8748e-01,  1.9645e-01,\n",
      "          4.4845e-01,  2.6984e-01, -4.9865e-01],\n",
      "        [ 4.1702e-02,  1.2455e-01, -5.8595e-01,  1.5874e-01,  2.2725e-01,\n",
      "          4.8440e-01,  3.0223e-01, -4.3040e-01],\n",
      "        [ 2.4101e-02,  1.0570e-01, -5.8847e-01, -2.3316e-01,  3.1466e-01,\n",
      "          7.0383e-01,  4.5402e-01,  8.6728e-02],\n",
      "        [ 8.5640e-02,  5.7276e-02, -6.3800e-01,  3.2887e-01,  6.8150e-02,\n",
      "          3.8451e-01,  1.4650e-01, -7.5298e-01],\n",
      "        [ 2.8525e-02,  1.3148e-01, -5.5782e-01,  2.2439e-01,  2.1433e-01,\n",
      "          4.4147e-01,  2.7851e-01, -4.6953e-01],\n",
      "        [ 1.3711e-01,  4.5127e-02, -7.3009e-01,  4.5047e-02,  1.3570e-01,\n",
      "          6.0219e-01,  2.3178e-01, -4.5417e-01],\n",
      "        [ 4.1603e-02,  1.3257e-01, -5.7933e-01,  6.7681e-02,  2.5599e-01,\n",
      "          5.5188e-01,  3.6972e-01, -2.7152e-01],\n",
      "        [ 5.8984e-02,  9.9488e-02, -6.1856e-01, -1.0160e-01,  2.8079e-01,\n",
      "          6.5869e-01,  3.9971e-01, -1.0506e-01],\n",
      "        [ 8.8842e-02,  7.7790e-02, -6.3404e-01,  1.0312e-01,  2.2219e-01,\n",
      "          5.5366e-01,  2.9000e-01, -3.8793e-01],\n",
      "        [ 2.2396e-02,  1.3427e-01, -5.9218e-01, -1.2322e-02,  2.5702e-01,\n",
      "          5.7215e-01,  3.8423e-01, -2.2894e-01],\n",
      "        [ 8.1913e-02,  8.9144e-02, -6.6349e-01,  2.3738e-02,  1.9193e-01,\n",
      "          5.9329e-01,  3.0782e-01, -3.3660e-01],\n",
      "        [ 3.9033e-02,  1.1622e-01, -6.1265e-01, -1.2933e-01,  2.6817e-01,\n",
      "          6.5543e-01,  4.1520e-01, -8.3453e-02],\n",
      "        [ 7.7181e-02,  7.4606e-02, -6.2482e-01,  3.1640e-01,  8.4825e-02,\n",
      "          3.9365e-01,  1.6998e-01, -7.0633e-01],\n",
      "        [ 7.5051e-02,  9.8836e-02, -6.4872e-01, -1.5804e-01,  2.5429e-01,\n",
      "          7.0608e-01,  4.0379e-01, -5.7997e-02],\n",
      "        [ 7.1156e-02,  8.9925e-02, -6.2472e-01, -8.4110e-02,  2.9460e-01,\n",
      "          6.6387e-01,  3.8313e-01, -1.1944e-01],\n",
      "        [ 8.0069e-02,  9.5928e-02, -6.6453e-01, -9.5071e-02,  2.2513e-01,\n",
      "          6.6223e-01,  3.6384e-01, -1.8347e-01],\n",
      "        [ 2.8158e-02,  1.2120e-01, -5.9662e-01, -1.2712e-01,  2.7719e-01,\n",
      "          6.5052e-01,  4.2510e-01, -6.0378e-02],\n",
      "        [ 4.9628e-02,  1.1651e-01, -6.0107e-01,  7.7501e-02,  2.5346e-01,\n",
      "          5.4292e-01,  3.3506e-01, -3.3367e-01],\n",
      "        [ 2.3142e-02,  1.3432e-01, -5.6670e-01,  1.1243e-01,  2.5209e-01,\n",
      "          5.0385e-01,  3.4102e-01, -3.4308e-01],\n",
      "        [ 3.8297e-02,  1.1457e-01, -6.0829e-01, -1.3528e-01,  2.8506e-01,\n",
      "          6.6814e-01,  4.1589e-01, -5.2077e-02],\n",
      "        [ 7.5635e-02,  9.0443e-02, -6.1850e-01,  2.5564e-01,  1.2135e-01,\n",
      "          4.3447e-01,  2.2392e-01, -6.0254e-01],\n",
      "        [ 6.6747e-02,  8.2414e-02, -6.0085e-01,  3.1614e-01,  1.2692e-01,\n",
      "          3.9159e-01,  1.8156e-01, -6.6808e-01],\n",
      "        [ 9.9151e-02,  7.0966e-02, -6.6230e-01,  1.0095e-01,  1.9717e-01,\n",
      "          5.4526e-01,  2.6680e-01, -4.5298e-01],\n",
      "        [ 4.3581e-02,  9.9442e-02, -5.6921e-01,  3.3039e-01,  1.4018e-01,\n",
      "          3.6852e-01,  1.8922e-01, -6.5167e-01],\n",
      "        [ 1.2041e-01,  6.5485e-03, -6.4990e-01,  3.8491e-01,  2.4801e-02,\n",
      "          3.7257e-01,  9.7578e-02, -8.1570e-01],\n",
      "        [ 1.0439e-01,  6.8912e-02, -6.8197e-01,  1.0880e-02,  1.8561e-01,\n",
      "          6.1523e-01,  2.9612e-01, -3.3187e-01],\n",
      "        [ 5.9265e-02,  1.0758e-01, -6.1197e-01,  9.7367e-03,  2.5446e-01,\n",
      "          5.9033e-01,  3.6359e-01, -2.4892e-01],\n",
      "        [ 1.1817e-01,  5.4532e-02, -6.8814e-01,  1.1980e-01,  1.5366e-01,\n",
      "          5.4967e-01,  2.3322e-01, -4.9521e-01],\n",
      "        [ 7.0212e-02,  9.7882e-02, -6.2053e-01,  6.2698e-02,  2.3407e-01,\n",
      "          5.6635e-01,  3.2890e-01, -3.2400e-01],\n",
      "        [ 5.2637e-02,  1.1809e-01, -6.1468e-01, -4.9085e-02,  2.5999e-01,\n",
      "          6.2960e-01,  3.9035e-01, -1.6328e-01],\n",
      "        [ 3.0306e-02,  1.2185e-01, -5.8825e-01,  4.0797e-04,  2.7665e-01,\n",
      "          5.7205e-01,  3.7814e-01, -2.3071e-01],\n",
      "        [ 4.1756e-02,  1.1195e-01, -6.0738e-01, -5.9574e-02,  2.7260e-01,\n",
      "          6.1852e-01,  3.8870e-01, -1.6463e-01],\n",
      "        [ 7.7188e-02,  8.8128e-02, -6.3724e-01,  8.4909e-02,  2.2100e-01,\n",
      "          5.5336e-01,  2.9661e-01, -3.8497e-01],\n",
      "        [ 3.5201e-02,  1.1402e-01, -6.1475e-01, -5.7070e-02,  2.5696e-01,\n",
      "          6.0656e-01,  3.7552e-01, -1.9652e-01],\n",
      "        [ 4.5347e-02,  1.0954e-01, -5.9730e-01, -2.5513e-01,  3.0802e-01,\n",
      "          7.3885e-01,  4.7222e-01,  1.3545e-01],\n",
      "        [ 8.1449e-02,  5.4397e-02, -6.0150e-01,  3.7090e-01,  8.1200e-02,\n",
      "          3.6502e-01,  1.4331e-01, -7.3190e-01],\n",
      "        [ 7.7632e-02,  5.8330e-02, -6.2244e-01,  3.8571e-01,  2.8607e-02,\n",
      "          3.3982e-01,  1.2275e-01, -8.2235e-01],\n",
      "        [ 1.1930e-02,  1.2808e-01, -5.6970e-01, -8.5269e-02,  3.1157e-01,\n",
      "          6.1708e-01,  4.2365e-01, -8.6509e-02],\n",
      "        [ 8.5866e-02,  8.7758e-02, -6.5680e-01,  6.7149e-02,  1.9085e-01,\n",
      "          5.6544e-01,  2.9642e-01, -3.9024e-01],\n",
      "        [ 6.8297e-03,  1.4484e-01, -5.4712e-01,  4.9896e-02,  2.9732e-01,\n",
      "          5.3435e-01,  3.8760e-01, -2.3546e-01],\n",
      "        [ 1.1269e-01,  6.3032e-02, -6.7726e-01, -1.0947e-01,  2.6104e-01,\n",
      "          7.0817e-01,  3.3765e-01, -1.3927e-01],\n",
      "        [ 3.9292e-02,  1.1441e-01, -5.8782e-01, -1.0171e-01,  3.1682e-01,\n",
      "          6.5260e-01,  4.2449e-01, -5.9062e-02],\n",
      "        [ 6.1598e-02,  1.0319e-01, -6.4892e-01, -1.3429e-01,  2.4241e-01,\n",
      "          6.7582e-01,  3.8590e-01, -1.1437e-01],\n",
      "        [ 8.5731e-02,  7.5480e-02, -6.3855e-01,  2.4217e-01,  1.3151e-01,\n",
      "          4.4051e-01,  2.0799e-01, -6.3016e-01],\n",
      "        [ 1.1508e-01,  6.5556e-02, -6.8365e-01,  7.5518e-02,  1.5119e-01,\n",
      "          5.7815e-01,  2.7132e-01, -4.1874e-01],\n",
      "        [ 6.5061e-02,  1.0007e-01, -5.7931e-01,  2.0408e-01,  2.1998e-01,\n",
      "          4.7957e-01,  2.8263e-01, -4.4465e-01],\n",
      "        [ 7.2470e-02,  9.5767e-02, -6.1107e-01,  1.0969e-01,  2.3142e-01,\n",
      "          5.4365e-01,  3.1163e-01, -3.6248e-01],\n",
      "        [ 5.9977e-02,  1.0553e-01, -6.1747e-01,  1.5015e-01,  1.8890e-01,\n",
      "          4.9831e-01,  2.8224e-01, -4.5808e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 1  # Each RNN cell takes 1 bit as input\n",
    "hidden_size = 8 # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output\n",
    "grid_height = 4  # 4x2 grid of RNN cells\n",
    "grid_width= 2\n",
    "batch_size = 64\n",
    "\n",
    "model = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width,batch_size)\n",
    "\n",
    "# Input is a batch of binary sequences (batch_size, seq_len, input_size), reshaped to match grid size\n",
    "seq_len = grid_height * grid_width  # Number of bits equal to the grid size squared\n",
    "x = torch.randn(batch_size, seq_len, input_size)  # Random input, replace with binary input\n",
    "hidden_ext = torch.randn(batch_size, hidden_size)  # Random input, replace with binary input\n",
    "\n",
    "output, hidden = model(x,hidden_ext)\n",
    "print(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.5728\n",
      "Training finished.\n",
      "Test Accuracy: 72.72%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 1  # Each Lattice RNN cell takes 1 bit as input\n",
    "hidden_size = 64  # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output (e.g., 0 or 1)\n",
    "grid_height = 2  # Number of rows in the grid\n",
    "grid_width = 4   # Number of columns in the grid\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# Create a model instance\n",
    "model = BlockRNN(input_size, hidden_size, output_size, grid_height, grid_width, rounds,batch_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "test_size=0.2\n",
    "test_dataset_size=num_shots*test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(detection_array1, observable_flips, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs,batch_size,rounds)\n",
    "\n",
    "test(model, X_test, y_test,batch_size)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
