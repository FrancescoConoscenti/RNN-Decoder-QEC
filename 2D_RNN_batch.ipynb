{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import stim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "distance=3\n",
    "num_ancilla_qubits=8\n",
    "rounds=5\n",
    "\n",
    "surface_code_circuit = stim.Circuit.generated(\n",
    "    \"surface_code:rotated_memory_x\",\n",
    "    rounds=5,\n",
    "    distance=3,\n",
    "    after_clifford_depolarization=0.01,\n",
    "    after_reset_flip_probability=0.01,\n",
    "    before_measure_flip_probability=0.01,\n",
    "    before_round_data_depolarization=0.01)\n",
    "\n",
    "\n",
    "num_shots=64*100\n",
    "# Compile the sampler\n",
    "sampler = surface_code_circuit.compile_detector_sampler()\n",
    "# Sample shots, with observables\n",
    "detection_events, observable_flips = sampler.sample(num_shots, separate_observables=True)\n",
    "\n",
    "\n",
    "detection_events = detection_events.astype(int)\n",
    "detection_strings = [''.join(map(str, row)) for row in detection_events] #compress the detection events in a tensor\n",
    "detection_events_numeric = [[int(value) for value in row] for row in detection_events] # Convert string elements to integers (or floats if needed)\n",
    "detection_array = np.array(detection_events_numeric) # Convert detection_events to a numpy array\n",
    "print(detection_array[0])\n",
    "\n",
    "detection_array1 = detection_array.reshape(num_shots, rounds, num_ancilla_qubits) #first dim is the number of shots, second dim round number, third dim is the Ancilla \n",
    "print(detection_array1[0]) \n",
    "\n",
    "observable_flips = observable_flips.astype(int).flatten().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data_stim/detection_surface_r5.npy', detection_array1)\n",
    "np.save('data_stim/observable_surface_r5.npy', observable_flips)\n",
    "\n",
    "detection = np.load('data_stim/detection_surface_r5.npy')\n",
    "observable = np.load('data_stim/observable_surface_r5.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class LatticeRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,batch_size):\n",
    "        super(LatticeRNNCell, self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.hidden=hidden_size\n",
    "        self.fc_input = nn.Linear(input_size, input_size)\n",
    "        self.fc_hidden_double = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_hidden_single = nn.Linear(hidden_size, hidden_size)\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden_left, hidden_up):\n",
    "        # if hidden_left is not None and hidden_bottom is not None:\n",
    "        #     hidden = (hidden_left + hidden_bottom) / 2  # Average of left and bottom\n",
    "        # elif hidden_left is not None:\n",
    "        #     hidden = hidden_left\n",
    "        # elif hidden_bottom is not None:\n",
    "        #     hidden = hidden_bottom\n",
    "        # else:\n",
    "        #     hidden = torch.zeros(x.size(0), self.hidden_size).to(x.device)  # Initial hidden state\n",
    "        \n",
    "        #input fc net\n",
    "        #input=self.fc_input(x.type(torch.FloatTensor))\n",
    "\n",
    "        # Combine the hidden states from left and bottom\n",
    "        if hidden_left is not None and hidden_up is not None:\n",
    "            combined_hidden=torch.cat((hidden_left,hidden_up),1)\n",
    "            hidden=self.fc_hidden_double(combined_hidden.squeeze(0))\n",
    "\n",
    "        elif hidden_left is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_left)\n",
    "\n",
    "        elif hidden_up is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_up)\n",
    "            \n",
    "        else:\n",
    "            hidden=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "\n",
    "        x=x.squeeze(1).float()\n",
    "\n",
    "        # Update hidden state using current input and combined hidden state\n",
    "        hidden = self.rnn_cell(x, hidden)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "class LatticeRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width,batch_size):\n",
    "        super(LatticeRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.grid_height = grid_height\n",
    "        self.grid_width = grid_width \n",
    "        self.rnn_cells = nn.ModuleList([LatticeRNNCell(input_size, hidden_size,batch_size) for _ in range(grid_height * grid_width)])\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, hidden_ext):\n",
    "        # Initialize a grid of hidden states\n",
    "        grid = [[None for _ in range(self.grid_width)] for _ in range(self.grid_height)]\n",
    "        \n",
    "        # Reshape the input to match the grid size\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.reshape(batch_size, self.grid_height, self.grid_width)\n",
    "\n",
    "        for i in range(self.grid_height):\n",
    "            for j in range(self.grid_width):\n",
    "                input_bit = x[:,i,j].unsqueeze(1).unsqueeze(1) # Get the input for the current cell\n",
    "                if j==0 & i==0:\n",
    "                    hidden_left = hidden_ext\n",
    "                hidden_left = grid[i][j - 1] if j > 0 else None\n",
    "                hidden_up = grid[i - 1][j] if i > 0 else None\n",
    "\n",
    "                # Get the index for the current RNN cell\n",
    "                cell_index = i * self.grid_width + j\n",
    "                hidden = self.rnn_cells[cell_index](input_bit, hidden_left, hidden_up)\n",
    "\n",
    "                # Store the hidden state in the grid\n",
    "                grid[i][j] = hidden\n",
    "\n",
    "        # The output that matters is the hidden state from the top-right corner (i.e., grid[grid_size-1][grid_size-1])\n",
    "        bottom_right_hidden = grid[-1][-1]\n",
    "\n",
    "        # Pass the hidden state through the fully connected layer and sigmoid for binary output\n",
    "        hidden= self.fc_hidden(bottom_right_hidden)\n",
    "        output = self.fc_out(bottom_right_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "\n",
    "# RNN model\n",
    "class BlockRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width, rounds,batch_size):\n",
    "        super(BlockRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.fc_in = nn.Linear(input_size, input_size)\n",
    "        self.rnn_block = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width,batch_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, rounds):\n",
    "        #input=self.fc_in(x)\n",
    "\n",
    "        hidden_ext = torch.zeros(1,1,self.hidden_size)\n",
    "\n",
    "        for round in range (rounds):\n",
    "            input_block = x[:,round,:].unsqueeze(2)  # (1, 8)\n",
    "            out, hidden_ext = self.rnn_block(input_block, hidden_ext)\n",
    "\n",
    "        #I already use fc, sigmoid in the LatticeRNN\n",
    "        #out = self.fc_out(out)  # Use the last time-step's output, needed for changing the dimension of the output compared of input\n",
    "        #out = self.sigmoid(out)  # I need a Binary output\n",
    "        return out, hidden_ext\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs, batch_size,rounds):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    num_samples = len(X_train[:,0,0])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            # Create mini-batches\n",
    "            batch_x = torch.from_numpy(X_train[i:i + batch_size])\n",
    "            batch_y = torch.Tensor(y_train[i:i + batch_size])\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            #hidden = model.init_hidden(1)\n",
    "            output, hidden = model(batch_x, rounds)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.squeeze(1), batch_y)\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize (update weights)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for logging purposes\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print average loss after each epoch\n",
    "        avg_loss = running_loss / (num_samples // batch_size)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "\n",
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_batch(batch_x, batch_y, model, criterion, rounds):\n",
    "    # Forward pass\n",
    "    output, hidden = model(batch_x, rounds)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(output.squeeze(1), batch_y)\n",
    "\n",
    "    # Backward pass (compute gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    return loss.item(), [param.grad.clone() for param in model.parameters() if param.grad is not None]\n",
    "\n",
    "\n",
    "\n",
    "def train_rnn_parallel(model,  X_train, y_train, criterion, optimizer,  num_epochs, batch_size, rounds, n_jobs=4):\n",
    "    \n",
    "    num_samples = len(X_train[:,0,0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0  # Reset loss for each epoch\n",
    "\n",
    "        # Ensure model is in training mode\n",
    "        model.train()\n",
    "\n",
    "        # Split data into batches\n",
    "        batches = [(torch.from_numpy(X_train[i:i + batch_size]),\n",
    "                    torch.Tensor(y_train[i:i + batch_size]))\n",
    "                for i in range(0, num_samples, batch_size)]\n",
    "\n",
    "        # Parallel processing of batches\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_batch)(batch_x, batch_y, model, criterion, rounds)\n",
    "            for batch_x, batch_y in batches\n",
    "        )\n",
    "\n",
    "        # Aggregate results\n",
    "        optimizer.zero_grad()  # Clear gradients before aggregation\n",
    "        for loss, grads in results:\n",
    "            running_loss += loss\n",
    "            # Aggregate gradients manually\n",
    "            for param, grad in zip(model.parameters(), grads):\n",
    "                if grad.shape != param.grad.shape:\n",
    "                    raise ValueError(f\"Gradient shape {grad.shape} does not match parameter shape {param.grad.shape}.\")\n",
    "                if param.grad is None:\n",
    "                    param.grad = grad.clone()\n",
    "                else:\n",
    "                    param.grad += grad\n",
    "                    # Step optimizer after aggregating gradients\n",
    "                    optimizer.step()\n",
    "\n",
    "        # Log the epoch's loss\n",
    "        avg_loss = running_loss / len(batches)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_array_to_tensor(binary_array):\n",
    "    # Check if the input is a NumPy array, if not, convert it\n",
    "    if isinstance(binary_array, np.ndarray):\n",
    "        tensor = torch.from_numpy(binary_array).float()  # Convert NumPy array to float32 tensor\n",
    "    else:\n",
    "        # If not a NumPy array, convert it as before\n",
    "        tensor = torch.tensor([[int(bit) for bit in binary_array]], dtype=torch.float32)\n",
    "    return tensor.unsqueeze(0)  # Add batch dimension (batch_size = 1)\n",
    "\n",
    "\n",
    "def test(model, test_sequences, targets,batch_size):\n",
    "    model.eval()  # Set the model to evaluation mode (disable dropout, etc.)\n",
    "    correct = 0\n",
    "\n",
    "    hidden = None\n",
    "    num_samples = len(test_sequences[:,0,0])\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            \n",
    "            output=np.zeros(batch_size)\n",
    "            batch_x = torch.from_numpy(test_sequences[i:i + batch_size])\n",
    "            target = targets[i:i + batch_size]\n",
    "            rounds = len(batch_x[0,:,0])\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            #if hidden is None:\n",
    "                # Initialize hidden state for the first sequence (or batch)\n",
    "            #    hidden = model.init_hidden(batch_size=1)  # batch_size might vary depending on your use case\n",
    "            #else:\n",
    "                #hidden=hidden.detach() #you detach if you want to avoid that the gradient is propagated through the hidden states to avoid long training time and memory usage\n",
    "            \n",
    "            # Forward pass for prediction\n",
    "            output, hidden = model(batch_x, rounds)\n",
    "            prediction = torch.round(output)  # Convert probability to binary (0 or 1)\n",
    "            \n",
    "            for j in range(0,batch_size):\n",
    "                if prediction[j] == target[j]:\n",
    "                    correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_sequences)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.5774e-02,  6.1452e-03, -2.8162e-01, -2.3069e-01, -2.8030e-01,\n",
      "         -6.0371e-01, -3.1045e-01, -3.0555e-01],\n",
      "        [-3.7751e-02,  1.7409e-01, -3.1123e-01, -4.0855e-01,  2.2051e-02,\n",
      "         -3.1137e-01, -2.0655e-01,  5.2681e-02],\n",
      "        [ 1.1997e-01,  6.4790e-02, -2.9484e-01, -1.6595e-01, -2.2933e-01,\n",
      "         -7.1123e-01, -1.8910e-01, -3.3526e-01],\n",
      "        [-8.1683e-02,  1.2784e-01, -2.7036e-01, -4.4781e-01,  1.4168e-01,\n",
      "         -1.2323e-01, -2.2175e-01,  1.3827e-01],\n",
      "        [ 5.3059e-02, -1.1033e-01, -2.1423e-01, -2.2126e-01, -1.7744e-01,\n",
      "         -5.3392e-01, -3.2974e-01, -3.7633e-01],\n",
      "        [ 2.1995e-03, -1.9642e-02, -2.4527e-01, -2.9163e-01, -1.1151e-01,\n",
      "         -4.6054e-01, -3.0437e-01, -2.3576e-01],\n",
      "        [ 4.3264e-02,  6.8450e-03, -2.6418e-01, -2.4332e-01, -1.7939e-01,\n",
      "         -5.6250e-01, -2.8332e-01, -2.8475e-01],\n",
      "        [-3.8386e-02,  9.0195e-02, -2.8253e-01, -3.6233e-01, -4.4692e-02,\n",
      "         -3.6467e-01, -2.6946e-01, -6.3934e-02],\n",
      "        [-8.3496e-02,  1.4105e-01, -2.6560e-01, -4.7140e-01,  2.1299e-01,\n",
      "          1.6758e-03, -1.8521e-01,  2.4069e-01],\n",
      "        [-1.0293e-01,  8.2000e-02, -2.5877e-01, -4.4199e-01,  5.9639e-02,\n",
      "         -3.2982e-03, -2.9341e-01,  1.8861e-01],\n",
      "        [-8.9028e-02,  1.8889e-01, -3.1557e-01, -4.4826e-01,  2.2850e-02,\n",
      "         -1.7104e-01, -2.3914e-01,  1.7391e-01],\n",
      "        [-5.5017e-02,  7.3001e-02, -2.6807e-01, -3.8510e-01, -5.0420e-03,\n",
      "         -2.6363e-01, -2.8176e-01, -9.5497e-03],\n",
      "        [ 8.3354e-02,  2.2961e-02, -2.7215e-01, -1.9283e-01, -2.0299e-01,\n",
      "         -6.5546e-01, -2.4180e-01, -3.3829e-01],\n",
      "        [-9.4981e-03,  5.5267e-02, -2.8877e-01, -2.9645e-01, -1.9693e-01,\n",
      "         -5.0846e-01, -3.1903e-01, -1.9470e-01],\n",
      "        [ 4.2603e-02,  1.3130e-01, -3.1570e-01, -2.6468e-01, -1.5382e-01,\n",
      "         -6.1688e-01, -2.0382e-01, -2.0339e-01],\n",
      "        [-3.4316e-02,  8.3257e-02, -2.8021e-01, -3.4558e-01, -6.4398e-02,\n",
      "         -4.0102e-01, -2.8112e-01, -9.9431e-02],\n",
      "        [ 8.8351e-02, -6.7004e-02, -2.4754e-01, -1.7960e-01, -2.9553e-01,\n",
      "         -6.3969e-01, -3.3189e-01, -4.0946e-01],\n",
      "        [-3.2577e-02, -7.3844e-03, -2.4436e-01, -3.2310e-01, -7.9397e-02,\n",
      "         -3.7863e-01, -3.2016e-01, -1.6588e-01],\n",
      "        [-3.9316e-02,  2.2284e-02, -2.6439e-01, -3.2091e-01, -1.4083e-01,\n",
      "         -4.1803e-01, -3.4494e-01, -1.6542e-01],\n",
      "        [-1.2818e-02, -4.3531e-02, -2.3575e-01, -3.0509e-01, -1.4083e-01,\n",
      "         -4.0112e-01, -3.6282e-01, -2.2026e-01],\n",
      "        [-5.5485e-02,  1.0405e-01, -2.6342e-01, -4.2357e-01,  1.1929e-01,\n",
      "         -1.7625e-01, -2.2419e-01,  7.4889e-02],\n",
      "        [-5.0983e-02,  1.3466e-01, -2.8482e-01, -4.1499e-01,  6.7731e-02,\n",
      "         -2.4754e-01, -2.2522e-01,  5.5460e-02],\n",
      "        [-3.6187e-03,  1.2408e-01, -3.0084e-01, -3.3151e-01, -6.9090e-02,\n",
      "         -4.7327e-01, -2.2923e-01, -1.1111e-01],\n",
      "        [-7.5709e-02,  1.0077e-01, -2.7350e-01, -4.1916e-01,  5.4695e-02,\n",
      "         -1.8007e-01, -2.5176e-01,  8.1012e-02],\n",
      "        [-4.8276e-02, -1.9653e-03, -2.3103e-01, -3.6151e-01,  2.8460e-02,\n",
      "         -2.7467e-01, -2.9057e-01, -9.1009e-02],\n",
      "        [ 9.3621e-03, -3.0401e-02, -2.4185e-01, -2.8535e-01, -1.2003e-01,\n",
      "         -4.6373e-01, -3.0767e-01, -2.4844e-01],\n",
      "        [-8.4430e-02,  8.1795e-02, -2.6675e-01, -4.1956e-01,  3.6889e-02,\n",
      "         -1.3659e-01, -2.7578e-01,  9.6047e-02],\n",
      "        [-1.5096e-02,  4.3618e-02, -2.5827e-01, -3.3394e-01, -1.2877e-02,\n",
      "         -3.9909e-01, -2.5174e-01, -1.3587e-01],\n",
      "        [-6.5550e-02,  1.5423e-01, -2.9040e-01, -4.3367e-01,  8.0830e-02,\n",
      "         -1.9287e-01, -2.2537e-01,  1.1372e-01],\n",
      "        [ 1.2024e-02,  2.5391e-02, -2.7294e-01, -2.9654e-01, -1.7507e-01,\n",
      "         -4.7680e-01, -3.1440e-01, -2.0460e-01],\n",
      "        [ 3.1633e-02, -2.5323e-04, -2.5941e-01, -2.5219e-01, -1.5726e-01,\n",
      "         -5.5137e-01, -2.8127e-01, -2.8156e-01],\n",
      "        [ 1.6254e-01, -5.8355e-02, -2.6462e-01, -1.1733e-01, -3.6379e-01,\n",
      "         -7.3227e-01, -2.6743e-01, -4.5876e-01],\n",
      "        [-2.1663e-02,  3.2581e-02, -2.6747e-01, -3.1831e-01, -1.2406e-01,\n",
      "         -4.3353e-01, -3.1669e-01, -1.6794e-01],\n",
      "        [-4.8206e-02,  4.6011e-02, -2.5960e-01, -3.6839e-01, -3.5712e-02,\n",
      "         -3.0121e-01, -3.0735e-01, -6.4638e-02],\n",
      "        [-5.8292e-02, -5.4548e-02, -2.2129e-01, -3.3857e-01, -8.2341e-02,\n",
      "         -2.7554e-01, -3.7879e-01, -1.3882e-01],\n",
      "        [ 1.1765e-01, -1.0163e-01, -2.3275e-01, -1.4714e-01, -2.8273e-01,\n",
      "         -6.6509e-01, -3.0187e-01, -4.5798e-01],\n",
      "        [ 1.2573e-01, -1.2399e-01, -2.1228e-01, -1.2578e-01, -2.4082e-01,\n",
      "         -6.6186e-01, -2.9599e-01, -4.8637e-01],\n",
      "        [-2.7326e-02,  2.8768e-02, -2.6481e-01, -3.1565e-01, -1.2607e-01,\n",
      "         -4.3261e-01, -3.2605e-01, -1.7207e-01],\n",
      "        [-7.0489e-02,  7.8015e-02, -2.6457e-01, -4.0937e-01,  2.7977e-02,\n",
      "         -1.8201e-01, -2.8315e-01,  5.3022e-02],\n",
      "        [ 8.8354e-02, -1.0150e-02, -2.5921e-01, -1.9196e-01, -1.9978e-01,\n",
      "         -6.3808e-01, -2.5051e-01, -3.5578e-01],\n",
      "        [-8.8497e-02, -2.2342e-02, -2.1620e-01, -4.0211e-01,  4.3854e-02,\n",
      "         -3.3832e-02, -3.3454e-01,  6.6313e-02],\n",
      "        [ 8.7582e-02,  2.7040e-02, -2.8673e-01, -1.8881e-01, -2.7369e-01,\n",
      "         -6.7754e-01, -2.6051e-01, -3.4233e-01],\n",
      "        [-2.7156e-02,  1.0608e-02, -2.5532e-01, -3.1580e-01, -9.7535e-02,\n",
      "         -4.2313e-01, -3.1090e-01, -1.7941e-01],\n",
      "        [-5.1421e-02,  2.8134e-02, -2.5659e-01, -3.5555e-01, -6.8985e-02,\n",
      "         -3.1596e-01, -3.2806e-01, -8.9284e-02],\n",
      "        [ 3.2701e-02,  4.3709e-02, -2.7355e-01, -2.9225e-01, -1.0594e-01,\n",
      "         -4.9618e-01, -2.4443e-01, -2.0117e-01],\n",
      "        [-6.3027e-02,  9.1194e-02, -2.8883e-01, -3.7740e-01, -9.1813e-02,\n",
      "         -3.0721e-01, -3.1332e-01, -1.7035e-02],\n",
      "        [-2.9936e-02, -8.4318e-02, -1.9867e-01, -3.2580e-01,  4.6530e-03,\n",
      "         -2.8023e-01, -3.1915e-01, -1.7270e-01],\n",
      "        [ 4.7375e-02,  4.0890e-02, -2.8633e-01, -2.5987e-01, -2.0352e-01,\n",
      "         -5.6467e-01, -2.6834e-01, -2.4733e-01],\n",
      "        [-9.8150e-02,  7.9640e-02, -2.7169e-01, -4.0560e-01, -3.1591e-02,\n",
      "         -1.6543e-01, -3.2571e-01,  7.2147e-02],\n",
      "        [-7.9494e-02,  1.7278e-01, -2.7805e-01, -4.7449e-01,  2.2438e-01,\n",
      "         -4.9371e-02, -1.6470e-01,  2.3149e-01],\n",
      "        [-1.1490e-01,  1.7372e-01, -2.9256e-01, -4.7501e-01,  1.1836e-01,\n",
      "         -1.4586e-02, -2.2497e-01,  2.7004e-01],\n",
      "        [-1.0820e-01,  1.8267e-01, -2.9948e-01, -4.7497e-01,  1.0083e-01,\n",
      "         -3.1907e-02, -2.2475e-01,  2.6633e-01],\n",
      "        [ 1.0452e-01, -5.0168e-02, -2.5165e-01, -1.6306e-01, -2.7928e-01,\n",
      "         -6.6433e-01, -2.9605e-01, -4.1225e-01],\n",
      "        [ 2.7030e-02,  7.8587e-02, -2.7544e-01, -3.1234e-01, -3.9599e-02,\n",
      "         -4.7203e-01, -2.2129e-01, -1.6129e-01],\n",
      "        [-7.2577e-02,  2.0953e-01, -3.0915e-01, -4.6245e-01,  1.2861e-01,\n",
      "         -1.5463e-01, -1.8040e-01,  1.9558e-01],\n",
      "        [-1.1513e-02,  1.4748e-01, -3.2489e-01, -3.3162e-01, -1.4704e-01,\n",
      "         -5.0435e-01, -2.4735e-01, -1.0096e-01],\n",
      "        [ 8.4089e-02,  4.0014e-02, -2.8962e-01, -1.9310e-01, -2.5275e-01,\n",
      "         -6.7664e-01, -2.4502e-01, -3.2928e-01],\n",
      "        [-6.2345e-02, -6.2131e-02, -2.1076e-01, -3.4598e-01, -3.3606e-02,\n",
      "         -2.3411e-01, -3.6240e-01, -1.1769e-01],\n",
      "        [-2.2247e-02,  1.0777e-01, -2.8707e-01, -3.5710e-01, -2.1670e-02,\n",
      "         -3.9987e-01, -2.3797e-01, -7.5608e-02],\n",
      "        [-9.6216e-02,  5.6859e-02, -2.3715e-01, -4.4331e-01,  1.4287e-01,\n",
      "         -3.1102e-03, -2.6442e-01,  1.5953e-01],\n",
      "        [-6.4125e-02,  5.4204e-02, -2.4787e-01, -4.0055e-01,  7.8217e-02,\n",
      "         -2.0486e-01, -2.6283e-01,  1.1068e-02],\n",
      "        [-4.3568e-02,  1.7110e-01, -3.1258e-01, -3.9497e-01, -1.5436e-02,\n",
      "         -3.5102e-01, -2.3362e-01,  1.9206e-02],\n",
      "        [-5.4146e-03,  4.8324e-02, -2.7495e-01, -3.0580e-01, -1.3807e-01,\n",
      "         -4.6801e-01, -3.0569e-01, -1.8058e-01],\n",
      "        [-9.1138e-02,  1.2426e-01, -2.7070e-01, -4.5182e-01,  1.2863e-01,\n",
      "         -5.8591e-02, -2.2875e-01,  1.8710e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 1  # Each RNN cell takes 1 bit as input\n",
    "hidden_size = 8 # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output\n",
    "grid_height = 4  # 4x2 grid of RNN cells\n",
    "grid_width= 2\n",
    "batch_size = 64\n",
    "\n",
    "model = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width,batch_size)\n",
    "\n",
    "# Input is a batch of binary sequences (batch_size, seq_len, input_size), reshaped to match grid size\n",
    "seq_len = grid_height * grid_width  # Number of bits equal to the grid size squared\n",
    "x = torch.randn(batch_size, seq_len, input_size)  # Random input, replace with binary input\n",
    "hidden_ext = torch.randn(batch_size, hidden_size)  # Random input, replace with binary input\n",
    "\n",
    "output, hidden = model(x,hidden_ext)\n",
    "print(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(detection, observable, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mtrain_rnn_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m test(model, X_test, y_test,batch_size)\n",
      "Cell \u001b[1;32mIn[96], line 44\u001b[0m, in \u001b[0;36mtrain_rnn_parallel\u001b[1;34m(model, X_train, y_train, criterion, optimizer, num_epochs, batch_size, rounds, n_jobs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Aggregate gradients manually\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters(), grads):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m:\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match parameter shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 1  # Each Lattice RNN cell takes 1 bit as input\n",
    "hidden_size = 64  # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output (e.g., 0 or 1)\n",
    "grid_height = 2  # Number of rows in the grid\n",
    "grid_width = 4   # Number of columns in the grid\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "# Create a model instance\n",
    "model = BlockRNN(input_size, hidden_size, output_size, grid_height, grid_width, rounds,batch_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "test_size=0.2\n",
    "test_dataset_size=num_shots*test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(detection, observable, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "train_rnn_parallel(model, X_train, y_train, criterion, optimizer, num_epochs,batch_size,rounds)\n",
    "\n",
    "test(model, X_test, y_test,batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
