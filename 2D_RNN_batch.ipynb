{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 0]\n",
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import stim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "distance=3\n",
    "num_ancilla_qubits=8\n",
    "rounds=5\n",
    "\n",
    "surface_code_circuit = stim.Circuit.generated(\n",
    "    \"surface_code:rotated_memory_x\",\n",
    "    rounds=5,\n",
    "    distance=3,\n",
    "    after_clifford_depolarization=0.01,\n",
    "    after_reset_flip_probability=0.01,\n",
    "    before_measure_flip_probability=0.01,\n",
    "    before_round_data_depolarization=0.01)\n",
    "\n",
    "\n",
    "num_shots=1024*100\n",
    "# Compile the sampler\n",
    "sampler = surface_code_circuit.compile_detector_sampler()\n",
    "# Sample shots, with observables\n",
    "detection_events, observable_flips = sampler.sample(num_shots, separate_observables=True)\n",
    "\n",
    "\n",
    "detection_events = detection_events.astype(int)\n",
    "detection_strings = [''.join(map(str, row)) for row in detection_events] #compress the detection events in a tensor\n",
    "detection_events_numeric = [[int(value) for value in row] for row in detection_events] # Convert string elements to integers (or floats if needed)\n",
    "detection_array = np.array(detection_events_numeric) # Convert detection_events to a numpy array\n",
    "print(detection_array[0])\n",
    "\n",
    "detection_array1 = detection_array.reshape(num_shots, rounds, num_ancilla_qubits) #first dim is the number of shots, second dim round number, third dim is the Ancilla \n",
    "print(detection_array1[0]) \n",
    "\n",
    "observable_flips = observable_flips.astype(int).flatten().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data_stim/detection_surface_r5.npy', detection_array1)\n",
    "np.save('data_stim/observable_surface_r5.npy', observable_flips)\n",
    "\n",
    "detection = np.load('data_stim/detection_surface_r5.npy')\n",
    "observable = np.load('data_stim/observable_surface_r5.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class LatticeRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,batch_size):\n",
    "        super(LatticeRNNCell, self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.hidden=hidden_size\n",
    "        self.fc_input = nn.Linear(input_size, input_size)\n",
    "        self.fc_hidden_double = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_hidden_single = nn.Linear(hidden_size, hidden_size)\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden_left, hidden_up):\n",
    "        # if hidden_left is not None and hidden_bottom is not None:\n",
    "        #     hidden = (hidden_left + hidden_bottom) / 2  # Average of left and bottom\n",
    "        # elif hidden_left is not None:\n",
    "        #     hidden = hidden_left\n",
    "        # elif hidden_bottom is not None:\n",
    "        #     hidden = hidden_bottom\n",
    "        # else:\n",
    "        #     hidden = torch.zeros(x.size(0), self.hidden_size).to(x.device)  # Initial hidden state\n",
    "        \n",
    "        #input fc net\n",
    "        #input=self.fc_input(x.type(torch.FloatTensor))\n",
    "\n",
    "        # Combine the hidden states from left and bottom\n",
    "        if hidden_left is not None and hidden_up is not None:\n",
    "            combined_hidden=torch.cat((hidden_left,hidden_up),1)\n",
    "            hidden=self.fc_hidden_double(combined_hidden.squeeze(0))\n",
    "\n",
    "        elif hidden_left is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_left)\n",
    "\n",
    "        elif hidden_up is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_up)\n",
    "            \n",
    "        else:\n",
    "            hidden=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "\n",
    "        x=x.squeeze(1).float()\n",
    "\n",
    "        # Update hidden state using current input and combined hidden state\n",
    "        hidden = self.rnn_cell(x, hidden)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "class LatticeRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width,batch_size):\n",
    "        super(LatticeRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.grid_height = grid_height\n",
    "        self.grid_width = grid_width \n",
    "        self.rnn_cells = nn.ModuleList([LatticeRNNCell(input_size, hidden_size,batch_size) for _ in range(grid_height * grid_width)])\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, hidden_ext):\n",
    "        # Initialize a grid of hidden states\n",
    "        grid = [[None for _ in range(self.grid_width)] for _ in range(self.grid_height)]\n",
    "        \n",
    "        # Reshape the input to match the grid size\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.reshape(batch_size, self.grid_height, self.grid_width)\n",
    "\n",
    "        for i in range(self.grid_height):\n",
    "            for j in range(self.grid_width):\n",
    "                input_bit = x[:,i,j].unsqueeze(1).unsqueeze(1) # Get the input for the current cell\n",
    "                if j==0 & i==0:\n",
    "                    hidden_left = hidden_ext\n",
    "                hidden_left = grid[i][j - 1] if j > 0 else None\n",
    "                hidden_up = grid[i - 1][j] if i > 0 else None\n",
    "\n",
    "                # Get the index for the current RNN cell\n",
    "                cell_index = i * self.grid_width + j\n",
    "                hidden = self.rnn_cells[cell_index](input_bit, hidden_left, hidden_up)\n",
    "\n",
    "                # Store the hidden state in the grid\n",
    "                grid[i][j] = hidden\n",
    "\n",
    "        # The output that matters is the hidden state from the top-right corner (i.e., grid[grid_size-1][grid_size-1])\n",
    "        bottom_right_hidden = grid[-1][-1]\n",
    "\n",
    "        # Pass the hidden state through the fully connected layer and sigmoid for binary output\n",
    "        hidden= self.fc_hidden(bottom_right_hidden)\n",
    "        output = self.fc_out(bottom_right_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "\n",
    "# RNN model\n",
    "class BlockRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width, rounds,batch_size):\n",
    "        super(BlockRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.fc_in = nn.Linear(input_size, input_size)\n",
    "        self.rnn_block = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width,batch_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, rounds):\n",
    "        #input=self.fc_in(x)\n",
    "\n",
    "        hidden_ext = torch.zeros(1,1,self.hidden_size)\n",
    "\n",
    "        for round in range (rounds):\n",
    "            input_block = x[:,round,:].unsqueeze(2)  # (1, 8)\n",
    "            out, hidden_ext = self.rnn_block(input_block, hidden_ext)\n",
    "\n",
    "        #I already use fc, sigmoid in the LatticeRNN\n",
    "        #out = self.fc_out(out)  # Use the last time-step's output, needed for changing the dimension of the output compared of input\n",
    "        #out = self.sigmoid(out)  # I need a Binary output\n",
    "        return out, hidden_ext\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs, batch_size,rounds):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    num_samples = len(X_train[:,0,0])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            # Create mini-batches\n",
    "            batch_x = torch.from_numpy(X_train[i:i + batch_size])\n",
    "            batch_y = torch.Tensor(y_train[i:i + batch_size])\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            #hidden = model.init_hidden(1)\n",
    "            output, hidden = model(batch_x, rounds)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.squeeze(1), batch_y)\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize (update weights)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for logging purposes\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print average loss after each epoch\n",
    "        avg_loss = running_loss / (num_samples // batch_size)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "\n",
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_batch(batch_x, batch_y, model, criterion, rounds):\n",
    "    # Forward pass\n",
    "    outputs, hidden = model(batch_x, rounds)\n",
    "    loss = criterion(outputs.squeeze(1), batch_y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Collect gradients\n",
    "    grads = [param.grad.clone() if param.grad is not None else torch.zeros_like(param)\n",
    "             for param in model.parameters()]\n",
    "    \n",
    "    # Reduce gradients over batch dimension\n",
    "    #reduced_grads = [grad.sum(dim=0, keepdim=True) if len(grad.shape) > 1 else grad for grad in grads]\n",
    "    \n",
    "    return loss.item(), grads\n",
    "\n",
    "\n",
    "def train_rnn_parallel(model,  X_train, y_train, criterion, optimizer,  num_epochs, batch_size, rounds, n_jobs=4):\n",
    "    \n",
    "    num_samples = len(X_train[:,0,0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0  # Reset loss for each epoch\n",
    "\n",
    "        # Ensure model is in training mode\n",
    "        model.train()\n",
    "\n",
    "        # Split data into batches\n",
    "        batches = [\n",
    "            (torch.from_numpy(X_train[i:i + batch_size]),\n",
    "            torch.Tensor(y_train[i:i + batch_size]))\n",
    "            for i in range(0, num_samples, batch_size)\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "        #results = [process_batch(batch_x, batch_y, model, criterion, rounds) for batch_x, batch_y in batches]\n",
    "        \n",
    "        results = Parallel(n_jobs=n_jobs)(delayed(process_batch)(batch_x, batch_y, model, criterion, rounds)for batch_x, batch_y in batches)   \n",
    "        end_time = time.time()\n",
    "\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Execution time: {elapsed_time:.6f} seconds\")\n",
    "        \n",
    "        # Aggregate results\n",
    "        optimizer.zero_grad()  # Clear gradients before aggregation\n",
    "        for loss, grads in results:\n",
    "            running_loss += loss\n",
    "            for param, grad in zip(model.parameters(), grads):\n",
    "                if grad.shape != param.shape:\n",
    "                    raise ValueError(f\"Gradient shape {grad.shape} does not match parameter shape {param.shape}.\")\n",
    "                if param.grad is None:\n",
    "                    param.grad = grad.clone()  # Initialize gradient\n",
    "                else:\n",
    "                    param.grad += grad  # Accumulate gradient\n",
    "\n",
    "        # Step optimizer after aggregating all gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the epoch's loss\n",
    "        avg_loss = running_loss / len(batches)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_array_to_tensor(binary_array):\n",
    "    # Check if the input is a NumPy array, if not, convert it\n",
    "    if isinstance(binary_array, np.ndarray):\n",
    "        tensor = torch.from_numpy(binary_array).float()  # Convert NumPy array to float32 tensor\n",
    "    else:\n",
    "        # If not a NumPy array, convert it as before\n",
    "        tensor = torch.tensor([[int(bit) for bit in binary_array]], dtype=torch.float32)\n",
    "    return tensor.unsqueeze(0)  # Add batch dimension (batch_size = 1)\n",
    "\n",
    "\n",
    "def test(model, test_sequences, targets,batch_size):\n",
    "    model.eval()  # Set the model to evaluation mode (disable dropout, etc.)\n",
    "    correct = 0\n",
    "\n",
    "    hidden = None\n",
    "    num_samples = len(test_sequences[:,0,0])\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            \n",
    "            output=np.zeros(batch_size)\n",
    "            batch_x = torch.from_numpy(test_sequences[i:i + batch_size])\n",
    "            target = targets[i:i + batch_size]\n",
    "            rounds = len(batch_x[0,:,0])\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            #if hidden is None:\n",
    "                # Initialize hidden state for the first sequence (or batch)\n",
    "            #    hidden = model.init_hidden(batch_size=1)  # batch_size might vary depending on your use case\n",
    "            #else:\n",
    "                #hidden=hidden.detach() #you detach if you want to avoid that the gradient is propagated through the hidden states to avoid long training time and memory usage\n",
    "            \n",
    "            # Forward pass for prediction\n",
    "            output, hidden = model(batch_x, rounds)\n",
    "            prediction = torch.round(output)  # Convert probability to binary (0 or 1)\n",
    "            \n",
    "            for j in range(0,batch_size):\n",
    "                if prediction[j] == target[j]:\n",
    "                    correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_sequences)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5669e-01, -4.9779e-02, -2.0291e-01, -1.5400e-01, -8.0922e-02,\n",
      "         -1.6268e-01,  6.3269e-02,  5.2135e-01],\n",
      "        [-2.2988e-01,  3.7740e-02, -1.9560e-01, -1.8423e-01, -7.2013e-02,\n",
      "         -1.4435e-01,  5.2990e-02,  4.9990e-01],\n",
      "        [-3.2207e-01, -5.1004e-02, -2.2464e-01, -2.1866e-01, -3.4816e-02,\n",
      "         -1.6592e-01,  1.3259e-01,  5.5436e-01],\n",
      "        [-4.2654e-01, -6.4160e-02, -2.4875e-01, -3.4933e-01,  7.5828e-02,\n",
      "         -1.9354e-01,  2.6326e-01,  6.0933e-01],\n",
      "        [-2.4684e-01, -1.2342e-01, -1.8999e-01, -1.1500e-01, -1.0884e-01,\n",
      "         -1.8973e-01,  1.3205e-01,  4.4450e-01],\n",
      "        [-2.5862e-01, -1.5941e-01, -1.7163e-01, -2.9586e-02, -1.7530e-01,\n",
      "         -1.8409e-01,  6.2434e-02,  4.7679e-01],\n",
      "        [-3.1748e-01, -7.7536e-02, -2.1027e-01, -2.1509e-01, -5.0297e-02,\n",
      "         -1.8491e-01,  2.7722e-01,  4.2625e-01],\n",
      "        [-4.5401e-01, -1.1944e-01, -2.6142e-01, -3.9519e-01,  1.1479e-01,\n",
      "         -2.2124e-01,  4.3475e-01,  5.2665e-01],\n",
      "        [-2.9787e-01, -2.1513e-01, -1.8354e-01, -9.5815e-02, -9.3353e-02,\n",
      "         -2.2047e-01,  1.5006e-01,  4.9165e-01],\n",
      "        [-2.7526e-01, -4.7681e-03, -2.0462e-01, -1.4351e-01, -1.0671e-01,\n",
      "         -1.3620e-01, -1.3432e-02,  5.7887e-01],\n",
      "        [-2.7783e-01, -8.8883e-03, -2.0634e-01, -1.9459e-01, -5.9296e-02,\n",
      "         -1.5641e-01,  8.3759e-02,  5.3189e-01],\n",
      "        [-2.6341e-01, -4.2456e-02, -2.1801e-01, -2.6584e-01,  1.3489e-02,\n",
      "         -1.8887e-01,  2.3161e-01,  4.4458e-01],\n",
      "        [-2.5711e-01,  2.1486e-02, -2.1638e-01, -2.3109e-01, -4.1451e-02,\n",
      "         -1.5225e-01,  1.4394e-01,  4.7417e-01],\n",
      "        [-3.7820e-01, -6.6249e-03, -2.4793e-01, -3.0268e-01,  1.4237e-02,\n",
      "         -1.5735e-01,  1.8407e-01,  5.9039e-01],\n",
      "        [-4.4366e-01, -3.1721e-01, -2.2215e-01, -1.4883e-01, -5.8930e-02,\n",
      "         -2.4281e-01,  2.9569e-01,  5.3959e-01],\n",
      "        [-1.7489e-01, -8.4756e-02, -1.6481e-01, -5.1035e-02, -1.6608e-01,\n",
      "         -1.7280e-01,  5.8127e-02,  4.0235e-01],\n",
      "        [-1.8464e-01,  7.9562e-02, -1.8832e-01, -9.6488e-02, -1.6676e-01,\n",
      "         -1.0608e-01, -8.1884e-02,  5.1503e-01],\n",
      "        [-1.6756e-01, -4.7053e-03, -1.6636e-01, -5.1456e-02, -1.7993e-01,\n",
      "         -1.3865e-01, -2.8318e-02,  4.5323e-01],\n",
      "        [-2.8671e-01, -3.4508e-02, -2.2110e-01, -2.0341e-01, -4.7216e-02,\n",
      "         -1.5964e-01,  9.4988e-02,  5.4363e-01],\n",
      "        [-3.4073e-01, -6.5470e-02, -2.3310e-01, -2.6527e-01, -6.0497e-03,\n",
      "         -1.8215e-01,  2.5883e-01,  4.8912e-01],\n",
      "        [-4.4351e-01, -2.4069e-01, -2.4402e-01, -2.6660e-01,  1.9791e-02,\n",
      "         -2.3536e-01,  4.5724e-01,  4.5352e-01],\n",
      "        [-2.1245e-01,  1.0601e-01, -2.0791e-01, -1.8529e-01, -7.4625e-02,\n",
      "         -1.0666e-01, -8.5925e-02,  5.8945e-01],\n",
      "        [-2.2351e-01, -1.4396e-01, -1.5779e-01, -5.3427e-04, -1.8476e-01,\n",
      "         -1.7736e-01, -5.0261e-02,  5.2056e-01],\n",
      "        [-1.1512e-01,  1.2089e-01, -1.5841e-01, -3.0658e-02, -2.1272e-01,\n",
      "         -9.2081e-02, -2.7212e-01,  5.7084e-01],\n",
      "        [-2.8058e-01, -1.3709e-01, -1.8868e-01, -1.1762e-01, -1.1093e-01,\n",
      "         -1.9340e-01,  1.9309e-01,  4.3001e-01],\n",
      "        [-2.5693e-01, -5.4435e-02, -1.9847e-01, -1.7954e-01, -7.1861e-02,\n",
      "         -1.7837e-01,  1.6045e-01,  4.4616e-01],\n",
      "        [-2.1767e-01,  3.2617e-02, -2.1119e-01, -1.9907e-01, -5.7921e-02,\n",
      "         -1.4397e-01,  2.9701e-02,  5.1768e-01],\n",
      "        [-2.4470e-01, -7.8206e-03, -2.1257e-01, -1.6484e-01, -1.1260e-01,\n",
      "         -1.4431e-01,  1.4074e-01,  4.3006e-01],\n",
      "        [-1.5375e-01, -5.6138e-02, -1.5176e-01, -4.6355e-02, -1.6091e-01,\n",
      "         -1.6802e-01,  1.8291e-02,  4.1296e-01],\n",
      "        [-1.4971e-01,  8.3826e-02, -1.6663e-01, -5.2757e-02, -1.7862e-01,\n",
      "         -1.0014e-01, -2.5179e-01,  5.9981e-01],\n",
      "        [-2.6676e-01, -1.8738e-04, -2.1164e-01, -1.4265e-01, -1.0011e-01,\n",
      "         -1.2797e-01, -4.7338e-02,  6.0327e-01],\n",
      "        [-3.6371e-01, -3.8129e-02, -2.4071e-01, -2.9593e-01,  1.3973e-02,\n",
      "         -1.7480e-01,  2.5673e-01,  5.2238e-01],\n",
      "        [-3.4274e-01,  5.2881e-02, -2.3374e-01, -3.2838e-01,  3.2958e-02,\n",
      "         -1.5367e-01,  2.1118e-01,  5.3961e-01],\n",
      "        [-2.2926e-01, -3.7417e-02, -1.8974e-01, -1.0738e-01, -1.3604e-01,\n",
      "         -1.5231e-01,  6.4857e-02,  4.6404e-01],\n",
      "        [-2.3100e-01, -1.7375e-01, -1.6594e-01,  2.1347e-02, -1.9886e-01,\n",
      "         -1.7443e-01, -6.3969e-02,  5.3646e-01],\n",
      "        [-4.9070e-01, -2.1748e-01, -2.4237e-01, -3.4595e-01,  9.2958e-02,\n",
      "         -2.4179e-01,  5.6895e-01,  4.4297e-01],\n",
      "        [-2.4008e-01,  6.4120e-02, -2.0195e-01, -1.9001e-01, -6.8813e-02,\n",
      "         -1.2905e-01,  2.1166e-02,  5.3791e-01],\n",
      "        [-2.9373e-01, -2.5824e-02, -2.2767e-01, -2.3877e-01, -2.3144e-02,\n",
      "         -1.6504e-01,  1.2555e-01,  5.3722e-01],\n",
      "        [-3.1860e-01, -4.4378e-02, -2.1699e-01, -2.1031e-01, -3.4589e-02,\n",
      "         -1.6484e-01,  1.0697e-01,  5.7077e-01],\n",
      "        [-2.6378e-01,  1.1473e-01, -2.2688e-01, -2.7193e-01, -1.5601e-02,\n",
      "         -1.1880e-01,  5.8298e-02,  5.5652e-01],\n",
      "        [-2.7361e-01, -1.2154e-01, -1.7012e-01, -5.0102e-02, -1.6431e-01,\n",
      "         -1.7585e-01,  6.4156e-02,  4.9389e-01],\n",
      "        [-2.8648e-01,  1.8014e-02, -2.1837e-01, -2.1446e-01, -4.8434e-02,\n",
      "         -1.4281e-01,  7.2644e-02,  5.5602e-01],\n",
      "        [-2.9980e-01, -1.6126e-01, -1.9662e-01, -1.4272e-01, -6.2916e-02,\n",
      "         -2.0606e-01,  1.2130e-01,  5.2859e-01],\n",
      "        [-3.6914e-01, -1.5363e-01, -2.2513e-01, -2.3832e-01, -7.1761e-04,\n",
      "         -2.1319e-01,  2.4268e-01,  5.3337e-01],\n",
      "        [-3.9804e-01, -1.0898e-01, -2.3147e-01, -2.9127e-01,  2.0121e-02,\n",
      "         -2.0361e-01,  3.6896e-01,  4.7141e-01],\n",
      "        [-2.8514e-01,  5.6293e-02, -2.1653e-01, -2.5343e-01, -2.7780e-02,\n",
      "         -1.4231e-01,  1.3379e-01,  5.1278e-01],\n",
      "        [-3.2541e-01, -4.0589e-03, -2.2634e-01, -2.7199e-01, -6.7158e-03,\n",
      "         -1.6588e-01,  1.7381e-01,  5.3386e-01],\n",
      "        [-1.9791e-01,  4.1056e-02, -1.7636e-01, -4.8855e-02, -2.0150e-01,\n",
      "         -1.1477e-01, -1.1668e-01,  5.3812e-01],\n",
      "        [-3.4307e-01, -3.8078e-02, -2.4162e-01, -2.7210e-01,  7.3685e-03,\n",
      "         -1.6709e-01,  1.3027e-01,  5.9715e-01],\n",
      "        [-2.5898e-01, -2.1217e-01, -1.7301e-01, -1.2588e-02, -1.5885e-01,\n",
      "         -1.9478e-01,  4.6306e-03,  5.3062e-01],\n",
      "        [-2.0705e-01, -6.8984e-02, -1.6704e-01, -1.9867e-02, -1.8271e-01,\n",
      "         -1.4847e-01, -1.0359e-01,  5.4843e-01],\n",
      "        [-2.8124e-01, -9.7053e-02, -2.1234e-01, -1.9952e-01, -4.3089e-02,\n",
      "         -1.9191e-01,  1.9148e-01,  4.6579e-01],\n",
      "        [-2.9476e-01, -1.6838e-01, -1.9444e-01, -1.0614e-01, -1.1403e-01,\n",
      "         -1.9627e-01,  1.8507e-01,  4.5045e-01],\n",
      "        [-2.2791e-01, -2.5065e-01, -1.5326e-01,  1.6272e-02, -1.9214e-01,\n",
      "         -2.2040e-01,  1.3369e-01,  3.8869e-01],\n",
      "        [-3.2361e-01, -6.8402e-02, -2.2168e-01, -2.1198e-01, -4.4844e-02,\n",
      "         -1.7319e-01,  1.7927e-01,  5.1469e-01],\n",
      "        [-3.4094e-01, -1.1548e-01, -2.3011e-01, -2.5372e-01,  6.7028e-03,\n",
      "         -2.0283e-01,  2.1597e-01,  5.3120e-01],\n",
      "        [-2.6000e-01, -1.3326e-01, -1.8064e-01, -9.3214e-02, -1.1568e-01,\n",
      "         -1.9048e-01,  8.9504e-02,  4.8676e-01],\n",
      "        [-3.8427e-01, -6.3476e-02, -2.3844e-01, -3.1412e-01,  3.4559e-02,\n",
      "         -1.9045e-01,  3.2728e-01,  4.9799e-01],\n",
      "        [-1.5679e-01,  3.3779e-03, -1.6592e-01, -2.1928e-02, -1.9787e-01,\n",
      "         -1.2594e-01, -1.5276e-01,  5.3110e-01],\n",
      "        [-3.0091e-01, -4.7118e-02, -1.9855e-01, -1.2034e-01, -1.1218e-01,\n",
      "         -1.4526e-01, -7.0572e-03,  5.9765e-01],\n",
      "        [-2.5022e-01, -2.5997e-01, -1.5173e-01,  5.1041e-03, -1.8488e-01,\n",
      "         -2.2797e-01,  1.6862e-01,  3.8455e-01],\n",
      "        [-2.5939e-01, -1.7733e-02, -2.0440e-01, -1.2303e-01, -1.1992e-01,\n",
      "         -1.3570e-01, -2.3212e-03,  5.5370e-01],\n",
      "        [-4.1492e-01, -1.1203e-01, -2.4167e-01, -3.1825e-01,  4.6504e-02,\n",
      "         -2.0716e-01,  3.7486e-01,  4.9918e-01],\n",
      "        [-3.6288e-01, -1.2191e-01, -2.1346e-01, -1.9252e-01, -4.8751e-02,\n",
      "         -1.8941e-01,  2.2438e-01,  5.1844e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 1  # Each RNN cell takes 1 bit as input\n",
    "hidden_size = 8 # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output\n",
    "grid_height = 4  # 4x2 grid of RNN cells\n",
    "grid_width= 2\n",
    "batch_size = 64\n",
    "\n",
    "model = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width,batch_size)\n",
    "\n",
    "# Input is a batch of binary sequences (batch_size, seq_len, input_size), reshaped to match grid size\n",
    "seq_len = grid_height * grid_width  # Number of bits equal to the grid size squared\n",
    "x = torch.randn(batch_size, seq_len, input_size)  # Random input, replace with binary input\n",
    "hidden_ext = torch.randn(batch_size, hidden_size)  # Random input, replace with binary input\n",
    "\n",
    "output, hidden = model(x,hidden_ext)\n",
    "print(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 7.748718 seconds\n",
      "Epoch 1/1, Loss: 0.6885\n",
      "Test Accuracy: 74.71%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 1  # Each Lattice RNN cell takes 1 bit as input\n",
    "hidden_size = 64  # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output (e.g., 0 or 1)\n",
    "grid_height = 2  # Number of rows in the grid\n",
    "grid_width = 4   # Number of columns in the grid\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "batch_size = 10240\n",
    "\n",
    "\n",
    "\n",
    "# Create a model instance\n",
    "model = BlockRNN(input_size, hidden_size, output_size, grid_height, grid_width, rounds,batch_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "test_size=0.2\n",
    "test_dataset_size=num_shots*test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(detection, observable, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "train_rnn_parallel(model, X_train, y_train, criterion, optimizer, num_epochs,batch_size,rounds)\n",
    "\n",
    "\n",
    "test(model, X_test, y_test,batch_size)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
