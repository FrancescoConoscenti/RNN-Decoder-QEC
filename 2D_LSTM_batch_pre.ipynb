{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import stim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "distance=3\n",
    "num_ancilla_qubits=8\n",
    "rounds=5\n",
    "\n",
    "surface_code_circuit = stim.Circuit.generated(\n",
    "    \"surface_code:rotated_memory_x\",\n",
    "    rounds=5,\n",
    "    distance=3,\n",
    "    after_clifford_depolarization=0.01,\n",
    "    after_reset_flip_probability=0.01,\n",
    "    before_measure_flip_probability=0.01,\n",
    "    before_round_data_depolarization=0.01)\n",
    "\n",
    "num_shots=100*64\n",
    "# Compile the sampler\n",
    "sampler = surface_code_circuit.compile_detector_sampler()\n",
    "# Sample shots, with observables\n",
    "detection_events, observable_flips = sampler.sample(num_shots, separate_observables=True)\n",
    "\n",
    "\n",
    "detection_events = detection_events.astype(int)\n",
    "detection_strings = [''.join(map(str, row)) for row in detection_events] #compress the detection events in a tensor\n",
    "detection_events_numeric = [[int(value) for value in row] for row in detection_events] # Convert string elements to integers (or floats if needed)\n",
    "detection_array = np.array(detection_events_numeric) # Convert detection_events to a numpy array\n",
    "print(detection_array[0])\n",
    "\n",
    "detection_array1 = detection_array.reshape(num_shots, rounds, num_ancilla_qubits) #first dim is the number of shots, second dim round number, third dim is the Ancilla \n",
    "print(detection_array1[0]) \n",
    "\n",
    "observable_flips = observable_flips.astype(int).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_size, layers_sizes, hidden_size):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        \n",
    "        # Define the input layer\n",
    "        self.input_layer = nn.Linear(input_size, layers_sizes[0])\n",
    "        \n",
    "        # Define hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(layers_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(layers_sizes[i], layers_sizes[i + 1]))\n",
    "        \n",
    "        # Define output layer\n",
    "        self.output_layer = nn.Linear(layers_sizes[-1], hidden_size)\n",
    "\n",
    "        # Define activation function (e.g., ReLU)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through input layer\n",
    "        #torch.transpose(x.squeeze(0), 1,0)\n",
    "\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        \n",
    "        # Pass through hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        # Pass through output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatticeRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,layers_sizes,batch_size):\n",
    "        super(LatticeRNNCell, self).__init__()\n",
    "        self.hidden=hidden_size\n",
    "        self.batch_size=batch_size\n",
    "        self.fc_input = nn.Linear(input_size, input_size)\n",
    "        #self.fc_hidden_double = nn.Linear(hidden_size*3, hidden_size)\n",
    "        self.multi_layer_fc = FullyConnectedNN(hidden_size*3, layers_sizes, hidden_size)\n",
    "        self.multi_layer_fc1 = FullyConnectedNN(hidden_size*3, layers_sizes, hidden_size)\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden_left, hidden_up, hidden_pre, hidden_pre1):\n",
    "        # if hidden_left is not None and hidden_bottom is not None:\n",
    "        #     hidden = (hidden_left + hidden_bottom) / 2  # Average of left and bottom\n",
    "        # elif hidden_left is not None:\n",
    "        #     hidden = hidden_left\n",
    "        # elif hidden_bottom is not None:\n",
    "        #     hidden = hidden_bottom\n",
    "        # else:\n",
    "        #     hidden = torch.zeros(x.size(0), self.hidden_size).to(x.device)  # Initial hidden state\n",
    "        \n",
    "        #input fc net\n",
    "        #input=self.fc_input(x.type(torch.FloatTensor))\n",
    "\n",
    "        \"\"\"\n",
    "        # Combine the hidden states from left and bottom\n",
    "        if hidden_left is not None and hidden_up is not None:\n",
    "            combined_hidden=torch.cat((hidden_left,hidden_up),1)\n",
    "            hidden=self.fc_hidden_double(combined_hidden.squeeze(0)).unsqueeze(0)\n",
    "\n",
    "        elif hidden_left is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_left.squeeze(0)).unsqueeze(0)\n",
    "\n",
    "        elif hidden_up is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_up.squeeze(0)).unsqueeze(0)\n",
    "        else:\n",
    "            hidden=torch.zeros(1,self.hidden, dtype=torch.float)\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_pre=torch.Tensor(hidden_pre)\n",
    "        hidden_pre1=torch.Tensor(hidden_pre1)\n",
    "\n",
    "        # Combine the hidden states from left and bottom\n",
    "        if hidden_left[0] is not None and hidden_up[0] is not None:\n",
    "            combined_hidden=torch.cat((hidden_left[0],hidden_up[0], hidden_pre),1)\n",
    "        elif hidden_left[0] is not None:\n",
    "            hidden_init_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden=torch.cat((hidden_left[0],hidden_init_up, hidden_pre),1)\n",
    "        elif hidden_up[0] is not None:\n",
    "            hidden_init_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden=torch.cat((hidden_init_left,hidden_up[0], hidden_pre),1)\n",
    "        else:\n",
    "            hidden_init_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            hidden_init_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden=torch.cat((hidden_init_left,hidden_init_up, hidden_pre),1)\n",
    "\n",
    "\n",
    "\n",
    "        if hidden_left[1] is not None and hidden_up[1] is not None:\n",
    "            combined_hidden1=torch.cat((hidden_left[1],hidden_up[1], hidden_pre1),1)\n",
    "        elif hidden_left[1] is not None:\n",
    "            hidden_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden1=torch.cat((hidden_left[1],hidden_up, hidden_pre1),1)\n",
    "        elif hidden_up[1] is not None:\n",
    "            hidden_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden1=torch.cat((hidden_left,hidden_up[1], hidden_pre1),1)\n",
    "        else:\n",
    "            hidden_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            hidden_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden1=torch.cat((hidden_left,hidden_up, hidden_pre1),1)\n",
    "\n",
    "\n",
    "        hidden=self.multi_layer_fc(combined_hidden)\n",
    "        hidden1=self.multi_layer_fc1(combined_hidden1)\n",
    "\n",
    "        x=x.squeeze(1).float()\n",
    "\n",
    "        # Update hidden state using current input and combined hidden state\n",
    "        hidden,hidden1 = self.lstm_cell(x, (hidden,hidden1))\n",
    "        \n",
    "        return hidden,hidden1\n",
    "\n",
    "class LatticeRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width,layers_sizes, batch_size):\n",
    "        super(LatticeRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.grid_height = grid_height\n",
    "        self.grid_width = grid_width \n",
    "        self.lstm_cells = nn.ModuleList([LatticeRNNCell(input_size, hidden_size, layers_sizes, batch_size) for _ in range(grid_height * grid_width)])\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, hidden_ext, hidden_ext1, grid):\n",
    "        \n",
    "        # Reshape the input to match the grid size\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.reshape(batch_size,self.grid_height, self.grid_width)\n",
    "\n",
    "        for i in range(self.grid_height):\n",
    "            for j in range(self.grid_width):\n",
    "\n",
    "                input_bit = x[:,i,j].unsqueeze(1).unsqueeze(1)  # Get the input for the current cell\n",
    "                (hidden_pre, hidden_pre1)=grid[i][j]\n",
    "\n",
    "                if i==0 and j==0:\n",
    "                    hidden_left=(hidden_ext,hidden_ext1)\n",
    "\n",
    "                if j > 0 :\n",
    "                    hidden_left = grid[i][j - 1]\n",
    "                else:\n",
    "                    hidden_left = (None,None)\n",
    "\n",
    "                if i > 0:\n",
    "                    hidden_up = grid[i-1][j]\n",
    "                else:\n",
    "                    hidden_up = (None,None)\n",
    "\n",
    "                \n",
    "\n",
    "                # Get the index for the current RNN cell\n",
    "                cell_index = i * self.grid_width + j\n",
    "                hidden, hidden1 = self.lstm_cells[cell_index](input_bit, hidden_left, hidden_up, hidden_pre, hidden_pre1)\n",
    "\n",
    "                # Store the hidden state in the grid\n",
    "                grid[i][j] = (hidden,hidden1)\n",
    "\n",
    "        # The output that matters is the hidden state from the top-right corner (i.e., grid[grid_size-1][grid_size-1])\n",
    "        final_hidden,  final_hidden1 = grid[-1][-1]\n",
    "\n",
    "        # Pass the hidden state through the fully connected layer and sigmoid for binary output\n",
    "        #hidden= self.fc_hidden(bottom_right_hidden)\n",
    "        output = self.fc_out(final_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output,  final_hidden, final_hidden1, grid\n",
    "    \n",
    "\n",
    "\n",
    "# RNN model\n",
    "class BlockRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width, rounds,layers_sizes,batch_size):\n",
    "        super(BlockRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.fc_in = nn.Linear(input_size, input_size)\n",
    "        self.rnn_block = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width, layers_sizes,batch_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, rounds):\n",
    "        #initialize the external hidden states\n",
    "        hidden_ext = torch.zeros(self.batch_size,self.hidden_size) # (1,hidden_size) hidden state of the previous round \n",
    "        hidden_ext1 = torch.zeros(self.batch_size,self.hidden_size)\n",
    "\n",
    "        #Initialize the previous hidden state for each element of the grid\n",
    "        grid = [[(hidden_ext,hidden_ext1) for _ in range(grid_width)] for _ in range(grid_height)]\n",
    "\n",
    "        for round in range (rounds):\n",
    "            input_block = x[:,round,:].unsqueeze(2)  # (1, 8) syndromes\n",
    "            out, hidden_ext, hidden_ext1,grid = self.rnn_block(input_block, hidden_ext, hidden_ext1, grid)\n",
    "\n",
    "        #I already use fc, sigmoid in the LatticeRNN\n",
    "        #out = self.fc_out(out)  # Use the last time-step's output, needed for changing the dimension of the output compared of input\n",
    "        #out = self.sigmoid(out)  # I need a Binary output\n",
    "        return out, hidden_ext[0]\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs, batch_size,rounds):\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    # If batch_size is None, set it to the full size of the dataset (no mini-batching)\n",
    "\n",
    "    num_samples = len(X_train[:,0,0])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # # Shuffle the dataset at the beginning of each epoch\n",
    "        # permutation = torch.randperm(num_samples)\n",
    "        # X_train = X_train[permutation]\n",
    "        # y_train = y_train[permutation]\n",
    "\n",
    "        # Mini-batch training loop\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            # Create mini-batches\n",
    "            batch_x = torch.from_numpy(X_train[i:i + batch_size])\n",
    "            batch_y = torch.Tensor(y_train[i:i + batch_size])\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            #hidden = model.init_hidden(1)\n",
    "            output, hidden= model(batch_x, rounds)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.squeeze(1), batch_y)\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize (update weights)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for logging purposes\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print average loss after each epoch\n",
    "        avg_loss = running_loss / (num_samples // batch_size)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_array_to_tensor(binary_array):\n",
    "    # Check if the input is a NumPy array, if not, convert it\n",
    "    if isinstance(binary_array, np.ndarray):\n",
    "        tensor = torch.from_numpy(binary_array).float()  # Convert NumPy array to float32 tensor\n",
    "    else:\n",
    "        # If not a NumPy array, convert it as before\n",
    "        tensor = torch.tensor([[int(bit) for bit in binary_array]], dtype=torch.float32)\n",
    "    return tensor.unsqueeze(0)  # Add batch dimension (batch_size = 1)\n",
    "\n",
    "\n",
    "def test(model, test_sequences, targets,batch_size):\n",
    "    model.eval()  # Set the model to evaluation mode (disable dropout, etc.)\n",
    "    correct = 0\n",
    "\n",
    "    hidden = None\n",
    "    num_samples = len(test_sequences[:,0,0])\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            \n",
    "            output=np.zeros(batch_size)\n",
    "            batch_x = torch.from_numpy(test_sequences[i:i + batch_size])\n",
    "            target = targets[i:i + batch_size]\n",
    "            rounds = len(batch_x[0,:,0])\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            #if hidden is None:\n",
    "                # Initialize hidden state for the first sequence (or batch)\n",
    "            #    hidden = model.init_hidden(batch_size=1)  # batch_size might vary depending on your use case\n",
    "            #else:\n",
    "                #hidden=hidden.detach() #you detach if you want to avoid that the gradient is propagated through the hidden states to avoid long training time and memory usage\n",
    "            \n",
    "            # Forward pass for prediction\n",
    "            output, hidden = model(batch_x, rounds)\n",
    "            prediction = torch.round(output)  # Convert probability to binary (0 or 1)\n",
    "            \n",
    "            for j in range(0,batch_size):\n",
    "                if prediction[j] == target[j]:\n",
    "                    correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_sequences)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.3828, -0.2308,  1.7810, -0.8717, -1.4502,  0.6449, -0.7657, -0.8595],\n",
      "        [ 0.4465,  0.2631, -0.3668,  0.8292, -1.7754,  0.5092,  0.4778, -1.2875],\n",
      "        [ 1.0268,  1.1072,  1.5153, -0.9435,  0.3933, -1.8176,  0.2264,  0.2579],\n",
      "        [-0.6780, -0.0839, -0.3555,  0.6451, -0.4284, -0.8158,  0.0876,  0.2736],\n",
      "        [ 1.5853,  0.2665, -1.1151,  1.3386, -1.7707, -2.8244,  1.5124,  2.1033],\n",
      "        [ 0.6984, -0.1893,  1.4000, -0.2501, -0.5194,  0.8266, -1.4237,  0.9600],\n",
      "        [ 0.8913,  0.4565,  1.4860, -0.3937, -1.1715, -0.4154,  0.4492,  0.9243],\n",
      "        [ 1.2519,  0.6777, -0.8730,  1.0986, -1.3260,  1.3426,  1.3883, -2.5794],\n",
      "        [-0.3898,  0.3868, -0.9496,  0.1856, -0.0998, -0.7518, -0.2682,  0.1330],\n",
      "        [ 2.5882,  1.1027, -0.3544, -1.6151,  2.1255, -0.5815, -1.3279, -1.7784],\n",
      "        [ 0.5494, -0.9577, -0.3117,  0.2608, -1.7650,  1.7253,  0.1102,  1.0625],\n",
      "        [-0.5680, -0.3036, -1.7354,  0.4713, -0.5229,  1.1842,  0.0547, -0.1647],\n",
      "        [ 1.2327, -0.4813, -0.1956,  1.9802,  1.1989, -1.4801,  0.1523,  0.0210],\n",
      "        [ 0.5256,  0.6733,  0.4154, -0.3748, -0.9716, -1.4833,  0.1865,  0.0612],\n",
      "        [-1.1244, -0.1225,  0.6750,  1.0189, -1.5555, -1.1354, -1.9284, -0.0192],\n",
      "        [ 0.1865, -0.8223, -0.0976, -1.0297, -0.6444, -1.3731,  1.1775, -0.5992],\n",
      "        [-0.6456, -0.3658,  2.1011, -0.5191, -0.9343,  1.2902, -0.6714,  1.1501],\n",
      "        [-2.0068,  0.7755,  0.6498, -0.8268,  0.7890,  0.1929,  0.5406, -1.3128],\n",
      "        [-0.7240, -0.5280, -0.5762,  1.7255, -0.0563,  1.0799, -0.5483,  0.8549],\n",
      "        [-1.5700,  1.2959,  1.3818,  1.1153, -1.5004,  0.7572, -0.8765,  0.4123],\n",
      "        [ 0.5127,  0.0990, -1.7063,  1.0551, -0.2475,  0.4016, -0.4885, -0.1523],\n",
      "        [-0.3498,  0.3561, -0.7724, -0.5706,  0.3459, -0.9437,  0.4339,  0.8477],\n",
      "        [ 0.0184,  0.5897, -1.0783,  1.3533,  0.4945, -0.4763,  2.0016,  1.6723],\n",
      "        [-0.4611, -0.5580,  0.0873, -1.3656,  0.1027,  0.9279, -1.1060, -0.6344],\n",
      "        [-0.3468,  0.0275,  1.0434, -0.1059, -1.8542,  1.0669,  0.8682,  1.2388],\n",
      "        [ 0.6954, -0.4763,  2.1606, -0.3785, -0.5586, -1.9729,  0.3398,  0.0818],\n",
      "        [ 0.8404,  0.4433, -0.5509,  1.5424, -0.8863,  0.9959,  0.5493, -2.2532],\n",
      "        [ 0.0276,  0.0652,  1.1484, -0.6024,  1.1750, -0.1306,  1.2220, -1.0100],\n",
      "        [ 0.9991, -0.0059, -0.7467,  0.8092, -0.9582, -0.9927, -1.5672, -0.9267],\n",
      "        [-1.0473, -1.0371,  0.4004, -1.2007,  0.8907,  0.6194,  0.6603,  1.9890],\n",
      "        [ 0.5893, -0.2476,  0.1348,  0.6652,  2.5980, -0.0181,  0.2002,  1.4861],\n",
      "        [ 0.8446,  0.7617, -3.4244, -0.6424, -0.1717, -0.3246, -0.2420, -0.2281],\n",
      "        [-0.2005, -1.4006, -0.7469, -0.3105, -0.0599, -0.3628, -0.2405,  0.1605],\n",
      "        [ 0.4949, -0.2278, -0.1983,  1.6834,  0.0995, -0.6391, -0.5630,  0.2992],\n",
      "        [-0.1204,  1.1686,  1.0174,  0.3741,  1.2830,  2.5020, -1.2624, -1.5656],\n",
      "        [-0.8182, -1.5691,  1.5591, -1.5971, -0.3978,  0.0595,  0.0582,  1.1579],\n",
      "        [-0.3529,  0.1285,  0.6301, -2.6360,  0.4038, -0.5366,  0.3052, -1.7522],\n",
      "        [-1.1453, -1.8544, -0.3883,  0.3661,  0.7657, -0.3803,  1.1267, -0.5792],\n",
      "        [ 0.2449,  0.1592,  0.5769,  1.0428, -0.3641,  0.0609, -1.1786, -0.5887],\n",
      "        [ 0.0855,  0.7159,  1.2587, -0.2045,  0.5927,  1.0238,  0.5567,  1.1106],\n",
      "        [ 0.4016,  1.5926, -0.9031, -0.3357, -0.1581, -0.4951, -0.1281, -0.6240],\n",
      "        [-1.6090, -0.4273, -0.4244,  2.5127,  0.7449,  1.3349,  0.9556,  0.3210],\n",
      "        [-0.0860,  0.3548, -0.6565, -0.1413,  1.0784,  0.2730,  1.1389,  0.1992],\n",
      "        [-0.6489, -0.9742,  0.1717,  0.8894, -0.2285,  0.1767, -0.0715,  1.2103],\n",
      "        [-1.5589,  0.4632,  0.4455, -1.7199, -1.7438,  1.3984,  0.3925,  0.2692],\n",
      "        [ 0.8245,  0.1407, -1.1013,  0.3466, -1.1815, -0.3633, -2.2030, -1.0833],\n",
      "        [-1.9341, -1.1892,  0.5173,  1.0335, -1.8927, -0.5441, -0.1542,  0.9611],\n",
      "        [ 0.3247, -0.9241,  1.2538, -0.1002,  1.8760,  0.0849, -1.0486,  0.2193],\n",
      "        [-0.5738,  0.1389, -1.0501,  0.3328,  1.3546,  0.7576,  0.6828,  0.6832],\n",
      "        [-1.2541, -0.1077,  1.0725, -0.8570,  1.1314, -2.8955,  1.2191, -0.1276],\n",
      "        [-0.1901,  0.7829, -0.2309, -1.2963, -0.0326,  0.7233, -1.1966,  1.3963],\n",
      "        [ 0.0053,  0.6035,  0.9373, -0.5958,  1.2005,  0.8988, -2.1411,  0.0189],\n",
      "        [ 1.9618, -0.8938, -0.7220, -0.5467, -1.1633,  0.0528, -0.3291, -2.1211],\n",
      "        [-0.3326,  0.0640, -0.0632, -1.1269, -0.0530, -0.5956, -1.1951,  1.8372],\n",
      "        [ 0.0371,  0.2297,  2.0628, -0.5348, -0.4672, -0.8964, -0.9498,  0.3648],\n",
      "        [-0.0975,  0.1792, -2.0734, -1.1555, -1.0143,  0.1281, -0.3755,  0.1119],\n",
      "        [-0.3495,  0.4255, -0.6298, -0.0759, -0.5046, -2.1574, -2.6200, -0.3938],\n",
      "        [ 0.0450, -1.0058,  1.7848, -0.8438, -0.4022, -0.1568, -1.3952, -0.5993],\n",
      "        [-1.0857, -2.3341, -1.7728,  0.7214,  0.2308,  1.8887, -0.5102, -0.1869],\n",
      "        [-1.5743, -2.1931,  0.6947, -0.8233, -0.8650,  1.8850, -2.0500,  1.9507],\n",
      "        [-0.3833, -1.2130, -0.0970,  0.3332,  0.1323,  0.9148, -0.8177, -1.2278],\n",
      "        [-0.6532, -0.5206,  1.1360,  1.0217,  0.5520, -1.6668, -0.1883, -1.3618],\n",
      "        [-0.8678,  0.2815,  1.1104, -0.8508,  0.0358,  0.7373, -0.3424,  1.0041],\n",
      "        [ 0.6109, -0.6386, -0.2202, -0.0137,  2.2948, -0.3031, -0.6357, -1.2897]]), tensor([[-9.5118e-01, -1.4326e+00, -8.1910e-01,  8.5465e-01,  1.5765e+00,\n",
      "          1.8603e+00,  1.8167e+00,  1.3124e+00],\n",
      "        [-4.6297e-01, -2.9326e+00,  5.2658e-01, -2.1157e-02, -5.1775e-01,\n",
      "          1.1613e+00, -8.6020e-01, -1.1234e+00],\n",
      "        [ 1.7030e+00,  9.1107e-02, -4.0309e-01, -5.2380e-02,  5.0782e-01,\n",
      "         -6.0190e-01,  2.3036e-01,  8.3036e-01],\n",
      "        [-3.5512e-01,  1.1149e+00,  5.8166e-01,  4.3844e-01,  1.9441e-01,\n",
      "          7.5987e-01,  5.4314e-02,  2.1743e+00],\n",
      "        [ 7.5790e-01,  1.4629e+00,  1.6519e+00,  4.8708e-01,  1.2262e+00,\n",
      "         -1.8175e+00,  9.7302e-01, -1.9843e+00],\n",
      "        [-6.8484e-01,  1.2199e+00,  1.3240e+00, -7.5858e-01, -2.6950e-01,\n",
      "          1.9656e+00, -1.3183e-01, -2.7544e-01],\n",
      "        [ 1.8338e+00, -3.7275e-01,  1.0682e+00, -2.5218e-01, -3.3316e-01,\n",
      "          7.7317e-02, -4.0747e-01, -3.6529e-01],\n",
      "        [ 2.2828e+00,  1.1460e+00, -4.2440e-02, -7.5921e-01,  9.9655e-01,\n",
      "          2.1897e-01,  2.1845e+00,  8.8559e-01],\n",
      "        [-7.4262e-01, -7.6799e-03,  3.9378e-01,  8.5080e-01, -6.9415e-01,\n",
      "         -2.0095e+00,  1.5566e+00, -6.9967e-01],\n",
      "        [ 1.1216e-01, -2.9385e+00, -7.4828e-01, -4.5018e-02,  5.5676e-01,\n",
      "          8.8833e-01, -1.5773e+00, -1.0808e+00],\n",
      "        [ 6.3329e-01, -3.6481e-01,  9.2615e-01, -2.7032e-01, -4.3734e-02,\n",
      "         -1.0334e+00,  2.4761e+00, -2.5479e-01],\n",
      "        [ 9.7875e-01,  1.1258e+00, -8.5629e-01,  1.2262e+00,  1.3611e+00,\n",
      "          5.7068e-01,  4.2727e-02, -7.0107e-01],\n",
      "        [ 2.6657e-01, -2.2491e-01,  1.6037e+00,  1.1426e+00,  1.2531e+00,\n",
      "         -1.0348e+00,  1.4698e+00, -8.9262e-01],\n",
      "        [-1.0809e+00, -1.2152e+00, -4.4052e-02, -9.2634e-01,  1.6512e+00,\n",
      "          7.0741e-01, -5.7113e-01, -5.7714e-03],\n",
      "        [ 7.7877e-01,  3.9796e-01,  6.9456e-02,  1.5903e-01,  9.3441e-01,\n",
      "         -7.3978e-01, -1.0228e+00, -1.0431e-01],\n",
      "        [ 1.2828e+00,  2.2339e+00,  7.8756e-01,  6.9528e-01, -6.2888e-01,\n",
      "         -1.9415e+00, -3.7667e-01, -5.2224e-01],\n",
      "        [ 9.1832e-01,  1.1608e-01,  1.5625e+00,  1.8580e-01, -4.5800e-02,\n",
      "          2.2323e-01,  8.2078e-01,  1.1815e+00],\n",
      "        [-1.4536e+00,  8.9174e-02,  1.7256e-01, -2.8989e-01,  1.5585e-01,\n",
      "         -3.4932e-01, -2.0543e+00,  1.1887e+00],\n",
      "        [ 7.0282e-01,  8.5110e-01,  6.0410e-02, -4.9487e-02, -1.7328e-01,\n",
      "         -9.0237e-02, -1.4428e+00, -1.0874e+00],\n",
      "        [ 6.2972e-01,  1.8950e-01, -5.0464e-01, -7.0115e-01, -1.6535e+00,\n",
      "          1.8744e-01,  3.3039e-01, -9.3083e-03],\n",
      "        [ 1.1731e+00, -1.2348e+00, -1.0615e-01,  2.6473e+00, -2.0002e-01,\n",
      "          7.1933e-01,  1.1740e+00,  2.3921e-02],\n",
      "        [ 2.1242e-01,  3.6266e-01,  3.4151e-01,  4.4183e-01,  3.2412e-01,\n",
      "         -2.4413e-01, -2.6918e-01,  1.7962e+00],\n",
      "        [ 1.2564e-01, -6.2455e-01,  7.9135e-01,  1.6410e+00, -5.3498e-02,\n",
      "          1.9914e-01, -8.5262e-01,  2.7970e+00],\n",
      "        [-5.7516e-02, -1.0762e+00, -7.4350e-01,  9.9614e-01,  7.8963e-01,\n",
      "         -4.4994e-01,  2.3555e-03,  2.9516e-01],\n",
      "        [ 6.9566e-01,  2.2220e-01, -1.8905e-01, -2.8628e+00, -2.1900e+00,\n",
      "         -2.2529e+00, -1.4450e+00,  6.7043e-02],\n",
      "        [-6.6785e-01, -1.1387e+00,  9.2462e-01, -1.4347e-02, -2.1569e-01,\n",
      "         -2.1957e-01,  2.2965e+00, -1.0043e+00],\n",
      "        [-9.0233e-01,  8.2615e-01,  3.8571e-01,  9.0300e-01, -9.6106e-01,\n",
      "          1.3666e+00,  1.2538e+00,  1.9953e-01],\n",
      "        [ 9.3720e-01,  4.5847e-01, -9.7894e-04, -9.1161e-01, -1.1223e+00,\n",
      "          9.1455e-01, -1.0898e+00, -4.7921e-01],\n",
      "        [-5.2928e-01, -8.5407e-01,  9.7260e-01,  1.5567e+00,  1.0899e+00,\n",
      "          7.9038e-01,  4.2815e-01,  4.2499e-02],\n",
      "        [-5.1318e-02, -3.4400e-04, -1.1825e+00,  2.6344e-01, -1.6564e+00,\n",
      "         -6.8919e-01, -6.3012e-01,  9.4868e-02],\n",
      "        [ 1.5066e+00, -7.6259e-01, -1.1214e+00, -1.6696e-02,  3.1793e-01,\n",
      "         -2.7065e+00, -8.4987e-01,  2.3035e-01],\n",
      "        [-1.3101e+00,  4.0469e-01, -4.8740e-01, -4.6511e-01,  5.7686e-01,\n",
      "         -8.2053e-01,  2.2474e-01, -1.2106e+00],\n",
      "        [-9.0251e-03,  5.4556e-01,  7.2452e-01,  5.5021e-01,  1.5616e+00,\n",
      "         -6.5776e-01,  1.2781e+00,  2.7005e-01],\n",
      "        [ 9.0085e-01, -3.0880e-02, -3.6589e-01,  1.7252e+00, -2.1598e-02,\n",
      "          1.8058e+00,  1.4330e+00, -1.9824e+00],\n",
      "        [-8.1502e-01, -1.3857e+00,  2.1565e-01, -1.4538e-01,  1.6495e+00,\n",
      "         -5.7408e-01, -9.3267e-01,  7.8984e-01],\n",
      "        [-1.4389e+00, -1.3188e+00, -1.3162e+00, -1.0949e+00,  1.6331e+00,\n",
      "         -1.1374e+00, -2.8301e-01,  3.4031e-01],\n",
      "        [ 7.0182e-01,  7.4010e-01,  1.1571e+00,  3.3557e-01,  4.2300e-02,\n",
      "          9.3698e-02,  4.8882e-01,  3.6279e-01],\n",
      "        [-3.0636e-01, -2.4224e+00,  1.7191e-01, -6.4372e-01,  2.2919e-01,\n",
      "          1.0515e+00, -5.2176e-01, -1.0276e+00],\n",
      "        [-4.8742e-01, -3.4712e-01,  2.0729e-01,  1.2337e-01, -6.6392e-01,\n",
      "          5.5628e-01,  2.2040e+00,  1.6774e-01],\n",
      "        [-1.0987e+00,  8.9613e-02, -1.3308e-01, -1.0090e+00,  3.0418e-01,\n",
      "         -3.3191e-01,  3.0211e-01, -5.5243e-01],\n",
      "        [-1.1662e+00,  1.4754e+00,  3.8003e-01, -5.2573e-02, -3.3015e-01,\n",
      "         -2.3185e-01,  1.6329e+00,  1.2710e+00],\n",
      "        [ 1.6937e-01, -9.8078e-02, -2.9090e-01,  2.3325e+00, -2.3751e+00,\n",
      "         -1.0650e+00, -6.2355e-01,  1.9740e-01],\n",
      "        [-2.7902e-01,  7.6830e-01, -1.7391e-01, -6.7826e-01, -5.4751e-01,\n",
      "          8.1186e-02,  2.3234e+00, -1.4911e-01],\n",
      "        [-5.5184e-01,  4.2431e-02, -1.2037e+00, -1.2412e-01, -3.5349e-01,\n",
      "          1.8923e+00,  1.0157e+00,  2.5132e-01],\n",
      "        [ 3.6286e-01,  6.8719e-01, -4.4662e-01,  6.4116e-01, -1.3871e+00,\n",
      "         -1.9350e+00,  6.2723e-01, -2.9004e-01],\n",
      "        [-5.6436e-01,  2.3518e+00, -1.6852e-01, -1.6732e-01,  4.4950e-01,\n",
      "          6.9960e-01,  1.9076e-01, -3.4451e-01],\n",
      "        [ 2.0813e-01, -8.2909e-01, -1.8728e-01, -1.6896e+00,  2.1574e+00,\n",
      "         -4.7613e-01,  7.4859e-01,  1.2357e+00],\n",
      "        [-7.2952e-01,  1.6890e-01,  6.0219e-02,  3.6756e-01, -4.2841e-01,\n",
      "          2.0888e+00,  1.2978e+00, -1.2846e+00],\n",
      "        [ 6.1654e-01, -1.2006e+00, -1.4953e+00,  2.0787e-01,  2.1175e-02,\n",
      "          3.5200e-01, -4.8455e-01, -1.1095e+00],\n",
      "        [ 8.7819e-01, -1.7582e+00,  1.7056e+00,  1.0458e+00,  5.5696e-01,\n",
      "          1.0640e+00,  1.3963e+00, -7.0910e-01],\n",
      "        [ 5.9123e-01,  8.2903e-01,  3.0792e-01, -1.0971e+00, -9.0447e-01,\n",
      "         -1.4693e+00,  1.6096e+00,  1.4874e+00],\n",
      "        [ 3.9979e-01,  1.0963e+00,  4.8207e-01,  3.5765e-01,  5.2090e-03,\n",
      "          6.3625e-01, -1.8668e+00,  1.4061e+00],\n",
      "        [-1.3633e+00,  2.9227e-03,  9.0503e-02,  2.0131e+00, -7.0928e-01,\n",
      "          4.8267e-01, -9.7905e-01, -6.5529e-01],\n",
      "        [-5.4506e-01,  9.3094e-01,  1.7964e-01, -3.2033e-01,  8.8160e-01,\n",
      "          8.1086e-01,  2.0393e-01,  1.1648e+00],\n",
      "        [-9.8008e-02,  1.3357e-02, -1.8079e-01, -1.3389e-01,  6.8812e-01,\n",
      "          1.6657e-01,  2.6544e-01,  2.7620e-01],\n",
      "        [-1.4462e+00, -7.4263e-02,  2.3141e-01,  5.3116e-01,  5.6619e-01,\n",
      "         -1.9926e+00,  2.3824e+00,  6.2192e-01],\n",
      "        [ 1.5772e+00,  6.8448e-01, -1.8603e-01, -8.3129e-01,  4.5157e-01,\n",
      "         -2.2417e+00,  7.3887e-01, -9.7622e-01],\n",
      "        [ 1.6423e-01,  6.9074e-01, -9.6498e-01,  2.8302e-01,  7.4134e-01,\n",
      "         -4.1798e-01, -1.0243e+00, -4.0315e-01],\n",
      "        [ 9.7861e-01,  1.2186e-02, -1.3668e+00, -1.9287e+00, -1.7883e+00,\n",
      "          6.9910e-01,  1.1056e+00, -5.2676e-01],\n",
      "        [ 7.0015e-01,  4.0876e-01, -1.6072e+00, -9.4576e-01,  4.7067e-01,\n",
      "         -4.2401e-01, -5.7761e-01, -9.6562e-01],\n",
      "        [ 6.8606e-01, -6.6934e-01, -7.4558e-01,  2.1775e-01,  5.8656e-01,\n",
      "         -6.8284e-01, -5.2386e-01,  1.0717e+00],\n",
      "        [-2.2851e+00, -4.5066e-01, -8.9364e-01, -1.4519e-01, -1.0322e+00,\n",
      "          2.0279e+00,  1.6865e-02, -3.8474e-01],\n",
      "        [ 6.9575e-01,  2.7658e-01,  8.6710e-01, -1.2254e+00,  1.0711e+00,\n",
      "          8.6753e-01, -1.3994e-01, -1.8883e+00],\n",
      "        [ 7.6365e-01,  4.2044e-01,  1.0387e+00, -1.0283e-01,  6.6854e-01,\n",
      "         -1.8886e+00, -1.4055e-01, -1.6435e-01]]))\n",
      "tensor([[ 0.1476,  0.0206,  0.1186,  0.0993, -0.1922, -0.1833,  0.0872,  0.2324],\n",
      "        [ 0.1253,  0.1085,  0.0285, -0.0255, -0.1595, -0.0228,  0.0174,  0.1452],\n",
      "        [ 0.1379,  0.0950,  0.0550, -0.0078, -0.1632, -0.0359,  0.0157,  0.1545],\n",
      "        [ 0.1223,  0.1445, -0.0048, -0.0689, -0.1368,  0.0438, -0.0580,  0.0882],\n",
      "        [ 0.1284,  0.1221,  0.0223, -0.0394, -0.1487,  0.0102, -0.0243,  0.1193],\n",
      "        [ 0.1333,  0.0290,  0.1075,  0.0734, -0.1947, -0.1681,  0.0827,  0.2223],\n",
      "        [ 0.1271,  0.1148,  0.0238, -0.0346, -0.1544, -0.0066, -0.0012,  0.1320],\n",
      "        [ 0.1420,  0.0423,  0.1073,  0.0849, -0.1818, -0.1425,  0.0732,  0.2188],\n",
      "        [ 0.1406,  0.0771,  0.0759,  0.0307, -0.1722, -0.0717,  0.0411,  0.1814],\n",
      "        [ 0.1332,  0.0981,  0.0420, -0.0201, -0.1659, -0.0394,  0.0235,  0.1513],\n",
      "        [ 0.1407,  0.0695,  0.0758,  0.0421, -0.1730, -0.0877,  0.0545,  0.1904],\n",
      "        [ 0.1324,  0.0917,  0.0580,  0.0126, -0.1646, -0.0442,  0.0239,  0.1659],\n",
      "        [ 0.1448,  0.0334,  0.1107,  0.0827, -0.1939, -0.1589,  0.0786,  0.2213],\n",
      "        [ 0.1358,  0.0717,  0.0745,  0.0173, -0.1742, -0.0798,  0.0427,  0.1798],\n",
      "        [ 0.1179,  0.1619, -0.0290, -0.1001, -0.1301,  0.0610, -0.0846,  0.0614],\n",
      "        [ 0.1320,  0.1051,  0.0466, -0.0170, -0.1550, -0.0144, -0.0046,  0.1418],\n",
      "        [ 0.1353,  0.0870,  0.0574,  0.0112, -0.1664, -0.0545,  0.0344,  0.1679],\n",
      "        [ 0.1350,  0.0359,  0.1052,  0.0637, -0.1872, -0.1487,  0.0778,  0.2174],\n",
      "        [ 0.1251,  0.0958,  0.0474, -0.0033, -0.1629, -0.0352,  0.0158,  0.1565],\n",
      "        [ 0.1380,  0.0645,  0.0830,  0.0418, -0.1767, -0.0933,  0.0522,  0.1926],\n",
      "        [ 0.1423,  0.0579,  0.0907,  0.0593, -0.1808, -0.1080,  0.0617,  0.2031],\n",
      "        [ 0.1297,  0.1139,  0.0313, -0.0294, -0.1536, -0.0041, -0.0059,  0.1328],\n",
      "        [ 0.1351,  0.0798,  0.0617,  0.0078, -0.1701, -0.0609,  0.0377,  0.1716],\n",
      "        [ 0.1318,  0.1017,  0.0453, -0.0065, -0.1594, -0.0250,  0.0115,  0.1513],\n",
      "        [ 0.1127,  0.1623, -0.0388, -0.0938, -0.1290,  0.0573, -0.0658,  0.0695],\n",
      "        [ 0.1458,  0.0581,  0.0877,  0.0380, -0.1823, -0.1066,  0.0600,  0.1944],\n",
      "        [ 0.1382,  0.0636,  0.0868,  0.0504, -0.1751, -0.0936,  0.0519,  0.1953],\n",
      "        [ 0.1387,  0.0699,  0.0767,  0.0279, -0.1719, -0.0813,  0.0491,  0.1849],\n",
      "        [ 0.1372,  0.0520,  0.0903,  0.0511, -0.1849, -0.1192,  0.0637,  0.2022],\n",
      "        [ 0.1463,  0.0455,  0.1044,  0.0739, -0.1841, -0.1385,  0.0756,  0.2149],\n",
      "        [ 0.1227,  0.1572, -0.0160, -0.0655, -0.1408,  0.0443, -0.0512,  0.0849],\n",
      "        [ 0.1375,  0.0385,  0.1077,  0.0780, -0.1871, -0.1510,  0.0769,  0.2204],\n",
      "        [ 0.1364,  0.0764,  0.0760,  0.0356, -0.1726, -0.0718,  0.0374,  0.1817],\n",
      "        [ 0.1386,  0.0642,  0.0838,  0.0462, -0.1804, -0.0991,  0.0549,  0.1945],\n",
      "        [ 0.1335,  0.0145,  0.1207,  0.0999, -0.1997, -0.1954,  0.0876,  0.2365],\n",
      "        [ 0.1186,  0.1692, -0.0366, -0.0975, -0.1279,  0.0657, -0.0788,  0.0587],\n",
      "        [ 0.1358,  0.0829,  0.0622,  0.0044, -0.1658, -0.0554,  0.0323,  0.1683],\n",
      "        [ 0.1110,  0.1915, -0.0707, -0.1284, -0.1195,  0.0846, -0.1052,  0.0289],\n",
      "        [ 0.1305,  0.1138,  0.0319, -0.0249, -0.1537, -0.0036, -0.0092,  0.1332],\n",
      "        [ 0.1415,  0.0051,  0.1275,  0.1103, -0.1991, -0.2131,  0.0918,  0.2415],\n",
      "        [ 0.1456,  0.0510,  0.1013,  0.0600, -0.1795, -0.1168,  0.0596,  0.2042],\n",
      "        [ 0.1272,  0.1212,  0.0241, -0.0301, -0.1499,  0.0076, -0.0139,  0.1269],\n",
      "        [ 0.1506, -0.0153,  0.1386,  0.1290, -0.1910, -0.2512,  0.0921,  0.2500],\n",
      "        [ 0.1358,  0.0919,  0.0607,  0.0142, -0.1653, -0.0446,  0.0250,  0.1658],\n",
      "        [ 0.1358,  0.0939,  0.0571,  0.0081, -0.1574, -0.0360,  0.0183,  0.1605],\n",
      "        [ 0.1442,  0.0182,  0.1195,  0.0995, -0.1973, -0.1881,  0.0874,  0.2334],\n",
      "        [ 0.1364,  0.0846,  0.0641,  0.0103, -0.1680, -0.0587,  0.0385,  0.1726],\n",
      "        [ 0.1215,  0.1626, -0.0247, -0.0816, -0.1352,  0.0548, -0.0644,  0.0720],\n",
      "        [ 0.1289,  0.1112,  0.0339, -0.0220, -0.1568, -0.0131,  0.0024,  0.1399],\n",
      "        [ 0.1287,  0.1220,  0.0209, -0.0350, -0.1564, -0.0026, -0.0018,  0.1280],\n",
      "        [ 0.1416,  0.0727,  0.0772,  0.0364, -0.1717, -0.0801,  0.0482,  0.1863],\n",
      "        [ 0.1159,  0.1582, -0.0309, -0.0941, -0.1313,  0.0557, -0.0733,  0.0665],\n",
      "        [ 0.1356,  0.0420,  0.1005,  0.0597, -0.1865, -0.1376,  0.0707,  0.2111],\n",
      "        [ 0.1235,  0.1363,  0.0088, -0.0494, -0.1425,  0.0287, -0.0392,  0.1053],\n",
      "        [ 0.1283,  0.1305,  0.0134, -0.0576, -0.1452,  0.0220, -0.0353,  0.1070],\n",
      "        [ 0.1368,  0.0800,  0.0724,  0.0315, -0.1685, -0.0631,  0.0362,  0.1794],\n",
      "        [ 0.1429,  0.0716,  0.0809,  0.0411, -0.1781, -0.0881,  0.0478,  0.1871],\n",
      "        [ 0.1412,  0.0702,  0.0797,  0.0274, -0.1749, -0.0834,  0.0458,  0.1838],\n",
      "        [ 0.1209,  0.1517, -0.0117, -0.0664, -0.1363,  0.0466, -0.0528,  0.0882],\n",
      "        [ 0.1159,  0.1843, -0.0532, -0.1053, -0.1207,  0.0792, -0.0965,  0.0417],\n",
      "        [ 0.1423,  0.0397,  0.1053,  0.0762, -0.1894, -0.1474,  0.0774,  0.2189],\n",
      "        [ 0.1448,  0.0425,  0.1048,  0.0632, -0.1880, -0.1366,  0.0687,  0.2101],\n",
      "        [ 0.1292,  0.0819,  0.0619,  0.0089, -0.1701, -0.0656,  0.0376,  0.1722],\n",
      "        [ 0.1369,  0.0857,  0.0652,  0.0139, -0.1711, -0.0601,  0.0331,  0.1698]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 1  # Each RNN cell takes 1 bit as input\n",
    "hidden_size = 8 # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output\n",
    "grid_height = 4  # 4x2 grid of RNN cells\n",
    "grid_width= 2\n",
    "batch_size=64\n",
    "layers_sizes=[hidden_size*3,hidden_size*2,hidden_size ]\n",
    "\n",
    "model = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width, layers_sizes,batch_size)\n",
    "\n",
    "# Input is a batch of binary sequences (batch_size, seq_len, input_size), reshaped to match grid size\n",
    "seq_len = grid_height * grid_width  # Number of bits equal to the grid size squared\n",
    "x = torch.randn(batch_size, seq_len, input_size)  # Random input, replace with binary input\n",
    "hidden_ext = torch.randn(batch_size, hidden_size)  # Random input, replace with binary input\n",
    "hidden_ext1 = torch.randn(batch_size, hidden_size)  # Random input, replace with binary input\n",
    "grid = [[(hidden_ext,hidden_ext1) for _ in range(grid_width)] for _ in range(grid_height)]\n",
    "print(grid[0][0])\n",
    "\n",
    "output, hidden, hidden1,grid = model(x,hidden_ext,hidden_ext1,grid)\n",
    "print(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6069\n",
      "Epoch [2/10], Loss: 0.5614\n",
      "Epoch [3/10], Loss: 0.5610\n",
      "Epoch [4/10], Loss: 0.5607\n",
      "Epoch [5/10], Loss: 0.5543\n",
      "Epoch [6/10], Loss: 0.5228\n",
      "Epoch [7/10], Loss: 0.5230\n",
      "Epoch [8/10], Loss: 0.5211\n",
      "Epoch [9/10], Loss: 0.5159\n",
      "Epoch [10/10], Loss: 0.5059\n",
      "Training finished.\n",
      "Test Accuracy: 74.22%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 1  # Each Lattice RNN cell takes 1 bit as input\n",
    "hidden_size = 64 # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output (e.g., 0 or 1)\n",
    "grid_height = 4  # Number of rows in the grid\n",
    "grid_width = 2   # Number of columns in the grid\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "layers_sizes=[hidden_size*3,hidden_size*2,hidden_size ]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a model instance\n",
    "model = BlockRNN(input_size, hidden_size, output_size, grid_height, grid_width, rounds,layers_sizes,batch_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "test_size=0.2\n",
    "test_dataset_size=num_shots*test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(detection_array1, observable_flips, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs,batch_size,rounds)\n",
    "\n",
    "test(model, X_test, y_test,batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
