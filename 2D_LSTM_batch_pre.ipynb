{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1]\n",
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "import stim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "distance=3\n",
    "num_ancilla_qubits=8\n",
    "rounds=5\n",
    "\n",
    "surface_code_circuit = stim.Circuit.generated(\n",
    "    \"surface_code:rotated_memory_x\",\n",
    "    rounds=5,\n",
    "    distance=3,\n",
    "    after_clifford_depolarization=0.01,\n",
    "    after_reset_flip_probability=0.01,\n",
    "    before_measure_flip_probability=0.01,\n",
    "    before_round_data_depolarization=0.01)\n",
    "\n",
    "num_shots=100*64\n",
    "# Compile the sampler\n",
    "sampler = surface_code_circuit.compile_detector_sampler()\n",
    "# Sample shots, with observables\n",
    "detection_events, observable_flips = sampler.sample(num_shots, separate_observables=True)\n",
    "\n",
    "\n",
    "detection_events = detection_events.astype(int)\n",
    "detection_strings = [''.join(map(str, row)) for row in detection_events] #compress the detection events in a tensor\n",
    "detection_events_numeric = [[int(value) for value in row] for row in detection_events] # Convert string elements to integers (or floats if needed)\n",
    "detection_array = np.array(detection_events_numeric) # Convert detection_events to a numpy array\n",
    "print(detection_array[0])\n",
    "\n",
    "detection_array1 = detection_array.reshape(num_shots, rounds, num_ancilla_qubits) #first dim is the number of shots, second dim round number, third dim is the Ancilla \n",
    "print(detection_array1[0]) \n",
    "\n",
    "observable_flips = observable_flips.astype(int).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_size, layers_sizes, hidden_size):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        \n",
    "        # Define the input layer\n",
    "        self.input_layer = nn.Linear(input_size, layers_sizes[0])\n",
    "        \n",
    "        # Define hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(layers_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(layers_sizes[i], layers_sizes[i + 1]))\n",
    "        \n",
    "        # Define output layer\n",
    "        self.output_layer = nn.Linear(layers_sizes[-1], hidden_size)\n",
    "\n",
    "        # Define activation function (e.g., ReLU)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through input layer\n",
    "        #torch.transpose(x.squeeze(0), 1,0)\n",
    "\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        \n",
    "        # Pass through hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        # Pass through output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatticeRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,layers_sizes,batch_size):\n",
    "        super(LatticeRNNCell, self).__init__()\n",
    "        self.hidden=hidden_size\n",
    "        self.batch_size=batch_size\n",
    "        self.fc_input = nn.Linear(input_size, input_size)\n",
    "        #self.fc_hidden_double = nn.Linear(hidden_size*3, hidden_size)\n",
    "        self.multi_layer_fc = FullyConnectedNN(hidden_size*3, layers_sizes, hidden_size)\n",
    "        self.multi_layer_fc1 = FullyConnectedNN(hidden_size*3, layers_sizes, hidden_size)\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden_left, hidden_up, hidden_pre, hidden_pre1):\n",
    "        # if hidden_left is not None and hidden_bottom is not None:\n",
    "        #     hidden = (hidden_left + hidden_bottom) / 2  # Average of left and bottom\n",
    "        # elif hidden_left is not None:\n",
    "        #     hidden = hidden_left\n",
    "        # elif hidden_bottom is not None:\n",
    "        #     hidden = hidden_bottom\n",
    "        # else:\n",
    "        #     hidden = torch.zeros(x.size(0), self.hidden_size).to(x.device)  # Initial hidden state\n",
    "        \n",
    "        #input fc net\n",
    "        #input=self.fc_input(x.type(torch.FloatTensor))\n",
    "\n",
    "        \"\"\"\n",
    "        # Combine the hidden states from left and bottom\n",
    "        if hidden_left is not None and hidden_up is not None:\n",
    "            combined_hidden=torch.cat((hidden_left,hidden_up),1)\n",
    "            hidden=self.fc_hidden_double(combined_hidden.squeeze(0)).unsqueeze(0)\n",
    "\n",
    "        elif hidden_left is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_left.squeeze(0)).unsqueeze(0)\n",
    "\n",
    "        elif hidden_up is not None:\n",
    "            hidden=self.fc_hidden_single(hidden_up.squeeze(0)).unsqueeze(0)\n",
    "        else:\n",
    "            hidden=torch.zeros(1,self.hidden, dtype=torch.float)\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_pre=torch.Tensor(hidden_pre)\n",
    "        hidden_pre1=torch.Tensor(hidden_pre1)\n",
    "\n",
    "        # Combine the hidden states from left and bottom\n",
    "        if hidden_left[0] is not None and hidden_up[0] is not None:\n",
    "            combined_hidden=torch.cat((hidden_left[0],hidden_up[0], hidden_pre),1)\n",
    "        elif hidden_left[0] is not None:\n",
    "            hidden_init_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden=torch.cat((hidden_left[0],hidden_init_up, hidden_pre),1)\n",
    "        elif hidden_up[0] is not None:\n",
    "            hidden_init_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden=torch.cat((hidden_init_left,hidden_up[0], hidden_pre),1)\n",
    "        else:\n",
    "            hidden_init_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            hidden_init_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden=torch.cat((hidden_init_left,hidden_init_up, hidden_pre),1)\n",
    "\n",
    "\n",
    "\n",
    "        if hidden_left[1] is not None and hidden_up[1] is not None:\n",
    "            combined_hidden1=torch.cat((hidden_left[1],hidden_up[1], hidden_pre1),1)\n",
    "        elif hidden_left[1] is not None:\n",
    "            hidden_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden1=torch.cat((hidden_left[1],hidden_up, hidden_pre1),1)\n",
    "        elif hidden_up[1] is not None:\n",
    "            hidden_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden1=torch.cat((hidden_left,hidden_up[1], hidden_pre1),1)\n",
    "        else:\n",
    "            hidden_left=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            hidden_up=torch.zeros(self.batch_size,self.hidden, dtype=torch.float)\n",
    "            combined_hidden1=torch.cat((hidden_left,hidden_up, hidden_pre1),1)\n",
    "\n",
    "\n",
    "        hidden=self.multi_layer_fc(combined_hidden)\n",
    "        hidden1=self.multi_layer_fc1(combined_hidden1)\n",
    "\n",
    "        x=x.squeeze(1).float()\n",
    "\n",
    "        # Update hidden state using current input and combined hidden state\n",
    "        hidden,hidden1 = self.lstm_cell(x, (hidden,hidden1))\n",
    "        \n",
    "        return hidden,hidden1\n",
    "\n",
    "class LatticeRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width,layers_sizes, batch_size):\n",
    "        super(LatticeRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.grid_height = grid_height\n",
    "        self.grid_width = grid_width \n",
    "        self.lstm_cells = nn.ModuleList([LatticeRNNCell(input_size, hidden_size, layers_sizes, batch_size) for _ in range(grid_height * grid_width)])\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, hidden_ext, hidden_ext1, grid):\n",
    "        \n",
    "        # Reshape the input to match the grid size\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.reshape(batch_size,self.grid_height, self.grid_width)\n",
    "\n",
    "        for i in range(self.grid_height):\n",
    "            for j in range(self.grid_width):\n",
    "\n",
    "                input_bit = x[:,i,j].unsqueeze(1).unsqueeze(1)  # Get the input for the current cell\n",
    "                (hidden_pre, hidden_pre1)=grid[i][j]\n",
    "\n",
    "                if i==0 and j==0:\n",
    "                    hidden_left=(hidden_ext,hidden_ext1)\n",
    "\n",
    "                if j > 0 :\n",
    "                    hidden_left = grid[i][j - 1]\n",
    "                else:\n",
    "                    hidden_left = (None,None)\n",
    "\n",
    "                if i > 0:\n",
    "                    hidden_up = grid[i-1][j]\n",
    "                else:\n",
    "                    hidden_up = (None,None)\n",
    "\n",
    "                \n",
    "\n",
    "                # Get the index for the current RNN cell\n",
    "                cell_index = i * self.grid_width + j\n",
    "                hidden, hidden1 = self.lstm_cells[cell_index](input_bit, hidden_left, hidden_up, hidden_pre, hidden_pre1)\n",
    "\n",
    "                # Store the hidden state in the grid\n",
    "                grid[i][j] = (hidden,hidden1)\n",
    "\n",
    "        # The output that matters is the hidden state from the top-right corner (i.e., grid[grid_size-1][grid_size-1])\n",
    "        final_hidden,  final_hidden1 = grid[-1][-1]\n",
    "\n",
    "        # Pass the hidden state through the fully connected layer and sigmoid for binary output\n",
    "        #hidden= self.fc_hidden(bottom_right_hidden)\n",
    "        output = self.fc_out(final_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output,  final_hidden, final_hidden1, grid\n",
    "    \n",
    "\n",
    "\n",
    "# RNN model\n",
    "class BlockRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, grid_height, grid_width, rounds,layers_sizes,batch_size):\n",
    "        super(BlockRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.fc_in = nn.Linear(input_size, input_size)\n",
    "        self.rnn_block = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width, layers_sizes,batch_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary output\n",
    "    \n",
    "    def forward(self, x, rounds):\n",
    "        #initialize the external hidden states\n",
    "        hidden_ext = torch.zeros(self.batch_size,self.hidden_size) # (1,hidden_size) hidden state of the previous round \n",
    "        hidden_ext1 = torch.zeros(self.batch_size,self.hidden_size)\n",
    "\n",
    "        #Initialize the previous hidden state for each element of the grid\n",
    "        grid = [[(hidden_ext,hidden_ext1) for _ in range(grid_width)] for _ in range(grid_height)]\n",
    "\n",
    "        for round in range (rounds):\n",
    "            input_block = x[:,round,:].unsqueeze(2)  # (1, 8) syndromes\n",
    "            out, hidden_ext, hidden_ext1,grid = self.rnn_block(input_block, hidden_ext, hidden_ext1, grid)\n",
    "\n",
    "        #I already use fc, sigmoid in the LatticeRNN\n",
    "        #out = self.fc_out(out)  # Use the last time-step's output, needed for changing the dimension of the output compared of input\n",
    "        #out = self.sigmoid(out)  # I need a Binary output\n",
    "        return out, hidden_ext[0]\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs, batch_size,rounds):\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    # If batch_size is None, set it to the full size of the dataset (no mini-batching)\n",
    "\n",
    "    num_samples = len(X_train[:,0,0])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # # Shuffle the dataset at the beginning of each epoch\n",
    "        # permutation = torch.randperm(num_samples)\n",
    "        # X_train = X_train[permutation]\n",
    "        # y_train = y_train[permutation]\n",
    "\n",
    "        # Mini-batch training loop\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            # Create mini-batches\n",
    "            batch_x = torch.from_numpy(X_train[i:i + batch_size])\n",
    "            batch_y = torch.Tensor(y_train[i:i + batch_size])\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            #hidden = model.init_hidden(1)\n",
    "            output, hidden= model(batch_x, rounds)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.squeeze(1), batch_y)\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize (update weights)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for logging purposes\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print average loss after each epoch\n",
    "        avg_loss = running_loss / (num_samples // batch_size)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_array_to_tensor(binary_array):\n",
    "    # Check if the input is a NumPy array, if not, convert it\n",
    "    if isinstance(binary_array, np.ndarray):\n",
    "        tensor = torch.from_numpy(binary_array).float()  # Convert NumPy array to float32 tensor\n",
    "    else:\n",
    "        # If not a NumPy array, convert it as before\n",
    "        tensor = torch.tensor([[int(bit) for bit in binary_array]], dtype=torch.float32)\n",
    "    return tensor.unsqueeze(0)  # Add batch dimension (batch_size = 1)\n",
    "\n",
    "\n",
    "def test(model, test_sequences, targets,batch_size):\n",
    "    model.eval()  # Set the model to evaluation mode (disable dropout, etc.)\n",
    "    correct = 0\n",
    "\n",
    "    hidden = None\n",
    "    num_samples = len(test_sequences[:,0,0])\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            \n",
    "            output=np.zeros(batch_size)\n",
    "            batch_x = torch.from_numpy(test_sequences[i:i + batch_size])\n",
    "            target = targets[i:i + batch_size]\n",
    "            rounds = len(batch_x[0,:,0])\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            #if hidden is None:\n",
    "                # Initialize hidden state for the first sequence (or batch)\n",
    "            #    hidden = model.init_hidden(batch_size=1)  # batch_size might vary depending on your use case\n",
    "            #else:\n",
    "                #hidden=hidden.detach() #you detach if you want to avoid that the gradient is propagated through the hidden states to avoid long training time and memory usage\n",
    "            \n",
    "            # Forward pass for prediction\n",
    "            output, hidden = model(batch_x, rounds)\n",
    "            prediction = torch.round(output)  # Convert probability to binary (0 or 1)\n",
    "            \n",
    "            for j in range(0,batch_size):\n",
    "                if prediction[j] == target[j]:\n",
    "                    correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_sequences)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-1.7831e+00, -1.2861e-02,  2.7256e-02,  1.0256e+00,  1.9320e-01,\n",
      "          1.8648e+00, -1.0008e+00,  1.1203e+00],\n",
      "        [-2.7818e-01, -6.2227e-02,  1.7751e+00,  1.0031e+00, -1.0483e+00,\n",
      "         -1.0366e-01, -3.0159e-01, -4.5727e-01],\n",
      "        [-1.9314e-01,  8.5172e-01,  6.6053e-01,  1.8302e-01, -9.9420e-01,\n",
      "         -2.0466e+00,  9.3604e-01,  1.0329e+00],\n",
      "        [-1.6410e+00,  9.8014e-01,  1.1443e+00,  5.7589e-01,  1.6729e+00,\n",
      "         -2.2717e-01,  2.7834e+00, -1.2360e-01],\n",
      "        [-4.0513e-01,  1.1818e+00,  1.6612e+00, -2.0996e+00,  1.0680e+00,\n",
      "         -9.7677e-01,  5.8972e-01, -3.6436e-01],\n",
      "        [-1.6314e-01,  2.9748e-01,  3.5344e-01, -2.5320e-01,  1.4887e+00,\n",
      "         -4.4349e-01,  1.3361e+00, -7.8714e-01],\n",
      "        [ 2.2218e-01,  1.1774e+00,  1.1248e+00, -3.6518e-01, -5.4967e-01,\n",
      "         -1.8669e-01, -1.7990e+00,  4.7357e-02],\n",
      "        [ 1.1321e+00, -5.6721e-01, -3.2382e-01, -8.6195e-01, -1.0904e+00,\n",
      "          1.5043e+00, -1.1762e+00, -1.1909e+00],\n",
      "        [ 1.6968e+00, -1.5837e+00, -1.9899e+00,  4.1669e-01,  3.1518e-01,\n",
      "          1.8948e+00,  3.3760e-01, -3.2058e-01],\n",
      "        [-6.9811e-01, -1.5847e-01,  1.0859e+00, -3.8897e-01,  4.3086e-01,\n",
      "         -1.5441e-01, -1.9439e-02, -1.2986e+00],\n",
      "        [ 3.2022e-01, -2.1503e+00,  4.2071e-01,  1.5634e+00,  1.2986e+00,\n",
      "         -5.1982e-01,  2.1789e-01,  9.7081e-01],\n",
      "        [-2.1350e-01,  8.4092e-01, -1.0715e+00,  1.3923e+00, -1.6522e+00,\n",
      "          4.3009e-01,  2.3264e-01, -4.6703e-01],\n",
      "        [-7.6836e-01,  5.7385e-01, -2.1333e+00, -5.6644e-01,  5.8186e-01,\n",
      "          2.7498e-01, -9.2351e-01, -1.9668e-01],\n",
      "        [-1.6759e-01, -4.7127e-02, -1.2656e+00, -1.7872e+00,  1.6557e+00,\n",
      "          5.8600e-01, -1.5815e-01, -1.0462e+00],\n",
      "        [-3.0464e-01,  1.2038e+00, -1.5106e+00, -8.6126e-01, -1.8423e+00,\n",
      "         -5.4681e-01,  1.1123e+00,  2.4657e-01],\n",
      "        [-1.8025e-01,  8.5913e-01,  7.8526e-01,  1.3835e+00,  1.0666e+00,\n",
      "          7.5583e-01,  1.9022e-01,  9.9766e-01],\n",
      "        [ 3.5544e-01,  5.6755e-01,  1.2592e+00, -5.9891e-01, -1.4721e+00,\n",
      "          5.3364e-01, -9.2084e-02,  5.1414e-01],\n",
      "        [-1.8998e+00,  8.1442e-01,  4.3189e-01, -1.3317e+00, -9.4717e-01,\n",
      "          2.7756e-01,  3.9580e-01,  3.1665e-01],\n",
      "        [ 1.4844e-02, -3.1996e-01,  1.2094e+00,  1.5116e-01,  2.2804e-01,\n",
      "         -9.5562e-02, -1.4739e+00, -9.5916e-01],\n",
      "        [-6.2345e-01,  7.7219e-01,  1.6448e+00,  2.0022e-01,  6.1370e-01,\n",
      "          9.6986e-01,  3.3846e-01,  2.0292e+00],\n",
      "        [-1.8162e+00, -3.0625e-01, -1.0273e-01,  2.0914e-01, -6.6834e-01,\n",
      "          7.1452e-01,  2.0716e+00,  1.7211e+00],\n",
      "        [-5.3852e-01, -1.0345e-01,  8.1267e-01,  9.9160e-02, -7.9610e-01,\n",
      "         -1.0609e+00,  1.8375e+00,  1.8028e-01],\n",
      "        [ 2.1515e-02, -8.2178e-01, -2.4949e-01,  1.0859e+00, -1.1222e+00,\n",
      "         -7.7595e-02, -1.2876e+00,  2.1977e-02],\n",
      "        [ 6.9920e-01, -1.4999e-02,  1.7333e-01, -5.4094e-01, -1.9032e-01,\n",
      "          9.6249e-01,  1.3806e+00,  4.8790e-01],\n",
      "        [-2.6118e-01, -9.7383e-01, -2.5932e-01, -7.9299e-01,  8.8913e-01,\n",
      "         -9.5036e-01,  8.2301e-01, -1.6079e+00],\n",
      "        [-6.9621e-01, -1.1017e+00, -1.2166e+00,  5.0134e-01,  1.0975e-01,\n",
      "          1.3045e+00,  2.0153e+00, -1.8504e-01],\n",
      "        [ 1.3473e+00, -3.5488e-02,  1.6708e+00,  1.4574e+00,  1.2909e+00,\n",
      "         -1.7762e+00,  1.0962e+00,  7.0006e-01],\n",
      "        [-8.4899e-01, -3.6032e-01,  8.8649e-02,  7.9229e-01, -1.0942e+00,\n",
      "          1.6885e+00,  5.3237e-02, -4.9707e-01],\n",
      "        [ 1.2432e+00, -7.3245e-01,  6.1981e-01,  2.6208e-03,  7.4188e-01,\n",
      "         -6.3857e-01, -1.4279e+00, -8.3662e-01],\n",
      "        [-6.9396e-01,  1.3623e+00, -4.5339e-01,  3.7154e-01,  7.9085e-01,\n",
      "          2.7981e-02,  1.8684e-01,  4.9626e-01],\n",
      "        [ 2.1633e-02,  3.9820e-01,  3.7922e-01, -9.8649e-01, -4.2668e-02,\n",
      "         -4.4570e-01,  8.3576e-01, -1.4726e+00],\n",
      "        [-1.0754e-01, -1.4462e+00, -3.3105e-01,  1.0041e+00,  9.7067e-01,\n",
      "         -2.7752e+00, -8.6550e-01,  1.2324e-01],\n",
      "        [-1.7187e+00, -2.2307e-01, -7.9191e-01, -9.3086e-01, -9.8342e-01,\n",
      "         -1.2783e+00, -1.8643e+00,  9.7456e-01],\n",
      "        [ 8.0250e-01,  1.4749e+00, -7.4503e-01,  8.4149e-01,  9.8432e-01,\n",
      "         -1.1065e+00, -3.2461e-01,  8.1954e-01],\n",
      "        [-2.8653e-01, -1.4129e+00, -3.1976e-01,  1.3296e+00, -5.0550e-01,\n",
      "         -3.9024e-01, -1.5501e+00, -8.0722e-02],\n",
      "        [-2.8780e+00, -6.4492e-02, -1.3208e+00, -3.8517e-01, -2.3358e-01,\n",
      "         -1.3440e+00,  1.1208e+00,  9.8492e-02],\n",
      "        [-1.9139e+00, -9.4023e-01, -5.7871e-01, -3.5775e-01, -1.6217e+00,\n",
      "         -1.8953e-01, -5.7033e-01,  2.2601e-01],\n",
      "        [-1.1847e-01, -1.3753e+00, -1.4058e-01, -2.2173e+00, -1.8209e-01,\n",
      "         -1.7120e+00, -1.8324e+00, -1.0525e+00],\n",
      "        [-3.2012e-01, -3.7463e-01,  5.9113e-01, -9.6755e-01, -1.0182e+00,\n",
      "          1.7550e-01,  1.0882e+00,  5.3767e-01],\n",
      "        [-9.0800e-01,  1.5996e-01, -5.3224e-02,  1.2403e+00,  1.6670e-01,\n",
      "         -5.4837e-02,  2.2441e+00, -8.8784e-01],\n",
      "        [ 9.2994e-02, -8.7039e-01,  4.4855e-01,  5.7633e-01,  9.5414e-01,\n",
      "          1.6963e+00, -1.7056e+00, -6.3228e-02],\n",
      "        [ 1.3626e-01, -1.2676e+00, -4.3513e-01,  4.5510e-02, -3.0272e-01,\n",
      "          9.7688e-01,  3.2573e-01, -1.3874e+00],\n",
      "        [ 5.9027e-01, -2.9380e+00, -2.2675e-01, -2.6303e-01,  1.5422e-02,\n",
      "         -1.3350e+00,  4.9206e-01,  3.8854e-01],\n",
      "        [ 9.6360e-01, -1.5836e+00, -2.8296e-01, -1.0217e+00,  1.5869e+00,\n",
      "          5.0399e-01, -1.2561e-01, -2.9679e-01],\n",
      "        [-7.3945e-01, -2.1814e-01,  1.3652e+00,  4.6794e-01,  1.0726e-01,\n",
      "         -1.6531e+00, -1.8180e+00,  1.2942e+00],\n",
      "        [ 2.0779e+00, -1.4301e+00, -4.9578e-01,  1.5613e+00,  1.0008e+00,\n",
      "         -5.7477e-02, -1.7115e+00,  3.0889e-01],\n",
      "        [-3.6828e-02, -2.3869e+00, -3.1007e+00, -1.1385e+00,  5.8513e-01,\n",
      "         -1.7249e-01, -1.8424e+00, -4.2027e-01],\n",
      "        [ 1.2610e-01,  2.0282e-01,  1.7711e+00, -1.5583e+00,  1.3529e+00,\n",
      "          2.9952e+00, -3.8303e-01, -7.6354e-01],\n",
      "        [ 1.7983e+00,  1.5067e+00, -1.6219e+00, -8.3141e-01, -1.1474e+00,\n",
      "         -1.2945e+00,  9.5881e-01,  3.6838e-01],\n",
      "        [ 3.8215e-01, -1.2492e+00,  7.2483e-01,  1.6387e+00,  9.3202e-01,\n",
      "          1.5312e+00, -1.0765e+00,  8.3413e-01],\n",
      "        [ 1.0791e+00, -2.7229e-01,  1.4335e+00, -1.3073e+00,  1.3380e+00,\n",
      "         -1.3981e+00, -6.8519e-01,  4.5264e-01],\n",
      "        [-7.9555e-02, -1.3299e+00,  1.3473e+00,  7.9085e-01, -1.0720e+00,\n",
      "         -5.7329e-01, -7.5180e-01, -9.0625e-01],\n",
      "        [ 5.3233e-01, -6.2757e-01,  8.8196e-01, -1.8137e-01, -5.2497e-01,\n",
      "         -6.4278e-01,  1.2289e+00, -7.2845e-01],\n",
      "        [-1.0226e+00, -2.8534e-01,  1.4633e+00,  2.8427e-02,  8.6794e-01,\n",
      "          3.7051e-01, -1.1024e+00,  1.2278e+00],\n",
      "        [ 6.2070e-01,  5.9914e-01,  9.5671e-01, -6.7302e-01,  4.7885e-01,\n",
      "         -1.4383e+00,  3.7274e-02,  3.3913e-01],\n",
      "        [ 1.2157e+00,  2.8727e-01, -1.5557e-01, -1.2122e+00,  1.4610e-02,\n",
      "          1.5619e-01,  4.0482e-01, -9.1792e-01],\n",
      "        [ 1.5379e+00, -3.3107e-01,  3.5969e-01, -6.9421e-01,  2.4277e-01,\n",
      "         -7.6959e-01, -8.1042e-01, -8.1775e-02],\n",
      "        [-3.1248e-01,  1.3269e+00, -2.7290e-01,  1.8690e+00,  1.6466e+00,\n",
      "          1.4088e+00,  4.6431e-01, -9.4643e-01],\n",
      "        [ 1.5615e-01,  1.2339e+00,  7.1973e-01,  9.0863e-01, -1.7433e+00,\n",
      "         -3.1742e-01,  3.5578e-01,  3.9188e-01],\n",
      "        [-2.2871e+00,  1.7166e+00, -5.5143e-01, -9.3244e-01,  7.9872e-01,\n",
      "         -9.7204e-01,  1.9730e+00,  1.5924e+00],\n",
      "        [ 4.4441e-01,  6.1484e-01,  1.0763e+00, -5.7633e-01,  1.5516e+00,\n",
      "          9.8933e-02, -1.2669e+00,  5.7471e-01],\n",
      "        [-1.1258e+00, -1.3480e-01,  7.0623e-02,  8.2723e-02, -3.4879e-01,\n",
      "          8.8163e-02,  4.5600e-03, -1.3880e+00],\n",
      "        [-2.4673e-01, -1.0920e+00, -5.7658e-01, -9.4732e-01,  3.2389e-01,\n",
      "          3.2773e-01, -1.4244e+00, -6.4218e-01],\n",
      "        [-3.8612e-01,  5.9018e-03, -1.1509e-01, -6.0207e-01, -1.8864e-01,\n",
      "         -4.2281e-01, -6.9238e-01,  1.1679e+00]]), tensor([[ 0.2677,  0.2168,  0.5602, -0.1662, -0.0915, -0.8968,  2.4445, -0.1813],\n",
      "        [ 0.5045, -1.1284, -0.5319, -0.1575, -0.6481, -0.9248, -0.4710,  0.5611],\n",
      "        [-1.8270, -0.2135, -1.8757,  0.8812,  0.0079, -2.1561,  0.4344, -0.0624],\n",
      "        [ 0.7392, -2.0676, -1.4407, -1.6347, -0.4849, -0.4548, -2.4681, -1.4885],\n",
      "        [ 1.7973,  0.0145,  0.5306,  0.8886,  0.9034, -1.7194,  0.5127,  0.1438],\n",
      "        [ 0.8791, -1.1513,  0.4962, -0.4779, -1.5108, -0.4120, -1.0949, -0.5445],\n",
      "        [-0.0249, -0.0545,  1.2240, -0.1161, -0.0380, -0.9636,  2.1373,  0.7732],\n",
      "        [ 0.0790, -1.8123, -2.0541, -0.5016, -0.4951,  0.7538, -0.6274,  1.9580],\n",
      "        [-0.9446,  0.1604,  0.2809, -0.3543,  1.1337,  0.6419,  1.6482,  0.4993],\n",
      "        [-0.3758, -0.9419, -0.9352, -1.3582, -0.3119, -0.2032, -0.1900, -2.7707],\n",
      "        [-0.2076,  0.0273, -1.6508, -1.6403, -1.1385,  0.0456, -0.6887, -0.1420],\n",
      "        [-0.9415, -0.1453, -0.3837,  0.1818,  1.5194, -0.7931,  0.6157, -1.6205],\n",
      "        [-2.0718,  0.1195, -1.0561, -0.6040,  0.0410,  0.2190, -0.0837,  2.6275],\n",
      "        [-1.2635,  1.0076, -1.4802,  0.7058,  2.7537, -0.0834, -0.1718, -1.6431],\n",
      "        [ 1.7857, -0.0999, -0.2478,  0.4544, -1.2379,  1.1018,  0.6950,  1.2995],\n",
      "        [ 0.9921, -0.0118,  1.0727, -0.8356,  0.7479, -0.4769,  0.5619,  0.3821],\n",
      "        [ 1.1119,  0.4042,  1.2415,  0.7174, -0.4511, -0.5773,  1.0988, -0.3518],\n",
      "        [ 1.6264,  0.2542, -1.1472, -1.3784,  0.1158,  0.4265,  0.5913, -0.3047],\n",
      "        [ 1.1672,  0.3949,  0.0901,  0.9972, -0.3843,  0.0610,  0.1582, -0.1299],\n",
      "        [ 1.2823,  0.6594,  1.7919,  0.2840,  0.0696,  1.2433,  0.0700,  1.8594],\n",
      "        [-0.0862, -1.3109,  0.9538, -0.1860, -1.7321, -0.9315,  0.0462, -0.9701],\n",
      "        [-0.8326,  1.8188, -0.2218, -0.7635, -0.7013, -0.9150,  1.5218,  0.0220],\n",
      "        [-0.6748,  0.7623,  0.4547,  0.4101,  2.4867,  0.6109, -1.5784,  0.7144],\n",
      "        [ 0.2776,  0.4339, -0.4083,  0.8810,  0.3500, -1.0027,  0.7890, -0.1266],\n",
      "        [-1.1756,  1.1505, -0.4566,  0.3105, -1.1769, -0.4889, -0.1644,  2.0887],\n",
      "        [ 0.9437, -1.2337, -2.5195,  0.2074, -0.3418, -0.5704,  2.3440, -0.9882],\n",
      "        [ 1.1723, -0.4782, -1.7026,  1.8634, -0.6385, -0.3538,  0.1885,  0.1439],\n",
      "        [ 1.0637, -0.7269, -0.1813,  2.1110, -0.2089, -0.2160, -0.0684, -0.6848],\n",
      "        [-1.3019,  1.9947,  0.9686,  1.3771,  0.4674, -0.5210, -0.1357,  2.0176],\n",
      "        [-0.3740,  0.7917, -1.7939,  1.2222,  0.5383, -0.6709,  0.0119,  1.3088],\n",
      "        [-0.4132, -1.5127, -0.3244,  0.3123, -0.7320,  0.3158, -0.6087, -1.3436],\n",
      "        [ 0.1785,  0.1546,  0.6006,  0.2997,  1.5632,  1.0893,  0.2647,  0.3764],\n",
      "        [ 0.4104,  0.2879, -0.0674,  0.4655, -0.2520,  1.9032,  1.0475, -0.0924],\n",
      "        [ 1.8475,  1.4513,  0.6418,  0.3559,  0.2651,  0.2806,  0.2691, -2.2172],\n",
      "        [ 1.0277, -1.1117,  1.3049,  0.1562,  0.4587,  2.1781,  0.5390,  0.6116],\n",
      "        [ 1.0175,  0.7214,  0.2579, -0.7964, -0.2435, -1.7729,  0.2088,  0.4331],\n",
      "        [ 0.3357, -0.9955, -0.7334, -0.8098, -1.2600,  0.9430, -1.2824,  1.1286],\n",
      "        [-0.9724, -0.4993, -0.6539, -2.3890, -0.5032,  1.1413, -0.0468,  2.7693],\n",
      "        [-0.2803, -0.6073, -0.7509,  0.4384,  1.4075,  0.4331,  0.3581,  0.3293],\n",
      "        [ 0.8734,  0.8835,  0.8744, -0.1436, -1.9127, -1.7925, -0.3897, -0.5489],\n",
      "        [-0.8398,  0.0450, -1.1414, -0.5002, -1.3382,  0.2690, -0.2207, -0.7813],\n",
      "        [-0.5109, -0.3898, -0.1266,  2.3805, -0.6044, -0.5825, -0.1670,  1.0539],\n",
      "        [ 0.8950, -1.0055, -0.6752,  0.9606, -1.8573,  0.9641, -0.1681,  0.7392],\n",
      "        [-2.5785, -0.5364,  1.2400, -0.0845,  0.2077,  0.5347,  0.4009, -0.3874],\n",
      "        [-0.9074,  1.4023, -1.5102,  0.6573, -0.8431, -0.7756,  0.4360,  0.3130],\n",
      "        [ 0.6135, -0.9501,  0.1933, -0.4194, -1.1448,  0.4681, -0.4872,  0.4302],\n",
      "        [-0.6996,  1.6236, -0.8587,  0.9018,  1.3767, -2.2330,  1.7260,  0.5146],\n",
      "        [-1.1384, -1.5170,  0.1793,  0.8555,  0.9336, -0.7499, -0.2176,  0.2798],\n",
      "        [-0.2331,  1.5559, -1.0099,  0.7311, -0.6131,  2.1349,  0.6012,  1.3304],\n",
      "        [ 0.5534,  1.2901, -1.0068, -0.9856, -1.4546, -0.2753,  1.4946,  0.6227],\n",
      "        [-0.1950, -0.7904, -0.8375, -0.0586, -0.2826, -0.3149, -0.1297, -0.2429],\n",
      "        [-0.9102, -1.3801, -0.7818, -1.1633,  1.5411, -1.5775, -0.1165, -0.2091],\n",
      "        [ 2.2835, -1.5408, -0.5222,  1.8448,  1.2520,  0.7823,  0.7965, -0.5293],\n",
      "        [ 0.7364,  0.1260,  0.9770,  1.1063, -1.3513, -0.2432,  1.5791,  0.0121],\n",
      "        [-1.1019, -0.0659, -1.1399, -0.1722,  1.2643,  0.3131, -0.5665,  0.4657],\n",
      "        [ 0.5578,  0.7632,  0.9895,  0.9023, -0.7555,  0.0500,  0.9288,  0.7455],\n",
      "        [ 1.4979,  2.0218, -0.6868, -0.8673, -1.4671,  0.6769, -1.2654, -0.7154],\n",
      "        [ 0.1351, -1.1661, -0.1497, -0.7807, -0.6204,  1.9553,  0.7464,  0.6165],\n",
      "        [-0.3422, -1.3525, -0.3065,  0.2981,  0.3099,  0.2895,  0.4800, -0.6761],\n",
      "        [-0.5192, -0.7324,  1.1471,  1.5952, -0.4937,  0.5168, -1.1517,  0.0118],\n",
      "        [ 1.7224, -1.1686, -0.7163,  1.5563, -1.1754,  0.2230, -0.8853,  1.5649],\n",
      "        [ 0.1544,  0.0158,  0.1265,  0.1998, -0.9783, -1.6310, -2.5171, -0.0490],\n",
      "        [ 0.1191, -1.3074, -0.6172, -0.4359,  0.7939, -0.1551, -1.1319, -2.2847],\n",
      "        [-1.1173,  1.6964, -0.1097,  2.6329,  0.3299, -2.3393,  0.9010,  1.8902]]))\n",
      "tensor([[-3.4672e-02,  2.8434e-02,  1.3218e-01,  5.6746e-02, -4.4250e-02,\n",
      "         -1.5603e-01, -3.0366e-02,  5.5164e-02],\n",
      "        [-6.2484e-02,  3.7614e-02,  1.3186e-01,  7.3438e-02, -4.7126e-02,\n",
      "         -2.0488e-01, -5.0151e-02,  5.6322e-02],\n",
      "        [-2.4160e-02,  9.9049e-03,  1.4592e-01,  4.3485e-02, -5.3567e-02,\n",
      "         -1.2987e-01, -1.2920e-02,  6.5502e-02],\n",
      "        [-7.8687e-02,  4.1654e-02,  1.2893e-01,  6.8162e-02, -4.9761e-02,\n",
      "         -1.9546e-01, -4.2598e-02,  6.3800e-02],\n",
      "        [-4.4963e-02,  2.3586e-02,  1.4015e-01,  5.4603e-02, -5.0497e-02,\n",
      "         -1.3939e-01, -2.0402e-02,  5.3900e-02],\n",
      "        [-1.1967e-01,  5.8585e-02,  1.2270e-01,  1.1356e-01, -5.2781e-02,\n",
      "         -3.1300e-01, -1.0333e-01,  6.6635e-02],\n",
      "        [-9.5264e-02,  4.4772e-02,  1.2732e-01,  1.0021e-01, -4.9363e-02,\n",
      "         -2.9867e-01, -9.5311e-02,  6.7471e-02],\n",
      "        [-9.6002e-02,  5.1793e-02,  1.2759e-01,  8.8807e-02, -5.2939e-02,\n",
      "         -2.4911e-01, -6.8725e-02,  6.1599e-02],\n",
      "        [-8.7145e-02,  4.0829e-02,  1.2574e-01,  9.0843e-02, -5.1963e-02,\n",
      "         -2.9101e-01, -8.7842e-02,  7.1176e-02],\n",
      "        [ 3.3212e-02, -2.7518e-03,  1.3489e-01,  1.3665e-02, -3.1590e-02,\n",
      "         -5.1634e-04,  2.7303e-02,  3.4761e-02],\n",
      "        [-9.6411e-02,  4.7054e-02,  1.2591e-01,  9.3330e-02, -4.9203e-02,\n",
      "         -2.7407e-01, -8.1384e-02,  6.4601e-02],\n",
      "        [ 6.9652e-02, -1.4958e-02,  1.3510e-01,  8.9028e-05, -2.4963e-02,\n",
      "          5.5984e-02,  4.3774e-02,  3.2227e-02],\n",
      "        [-7.3778e-02,  3.9169e-02,  1.3178e-01,  7.1608e-02, -5.0794e-02,\n",
      "         -2.1045e-01, -4.9765e-02,  6.3095e-02],\n",
      "        [-5.5713e-02,  2.7677e-02,  1.2801e-01,  5.8027e-02, -5.3216e-02,\n",
      "         -2.1203e-01, -4.5707e-02,  7.6302e-02],\n",
      "        [-9.5153e-02,  5.4436e-02,  1.2278e-01,  9.4488e-02, -4.8723e-02,\n",
      "         -2.6784e-01, -7.9197e-02,  6.2750e-02],\n",
      "        [-6.0666e-02,  3.2832e-02,  1.2983e-01,  6.8759e-02, -4.7609e-02,\n",
      "         -2.1597e-01, -5.4030e-02,  6.6926e-02],\n",
      "        [-8.4198e-02,  4.4096e-02,  1.2878e-01,  9.2767e-02, -4.7509e-02,\n",
      "         -2.6715e-01, -7.9142e-02,  5.9403e-02],\n",
      "        [-1.7916e-01,  7.5579e-02,  1.0067e-01,  2.1540e-01, -8.0279e-04,\n",
      "         -5.0085e-01, -2.3183e-01,  6.0156e-02],\n",
      "        [-8.4240e-02,  4.1275e-02,  1.3108e-01,  8.8554e-02, -5.0336e-02,\n",
      "         -2.5607e-01, -7.3674e-02,  6.0421e-02],\n",
      "        [-1.3645e-01,  6.2304e-02,  1.1342e-01,  1.3514e-01, -3.8169e-02,\n",
      "         -3.7062e-01, -1.3717e-01,  6.6431e-02],\n",
      "        [-2.2025e-02,  2.5100e-02,  1.3278e-01,  4.9556e-02, -3.9409e-02,\n",
      "         -1.1382e-01, -1.5636e-02,  4.4623e-02],\n",
      "        [-7.3552e-02,  4.0208e-02,  1.3281e-01,  8.1889e-02, -5.0627e-02,\n",
      "         -2.3123e-01, -6.2018e-02,  5.9100e-02],\n",
      "        [-5.9768e-02,  2.1299e-02,  1.3511e-01,  6.3059e-02, -5.1322e-02,\n",
      "         -2.2484e-01, -5.2363e-02,  6.9409e-02],\n",
      "        [-6.9242e-02,  3.3188e-02,  1.3754e-01,  7.2193e-02, -5.6306e-02,\n",
      "         -2.1329e-01, -4.9971e-02,  6.2782e-02],\n",
      "        [-1.4414e-02,  2.9794e-02,  1.3238e-01,  4.6002e-02, -4.3345e-02,\n",
      "         -9.5574e-02, -8.3857e-03,  4.7699e-02],\n",
      "        [-1.2845e-01,  5.1682e-02,  1.3313e-01,  1.3112e-01, -5.9483e-02,\n",
      "         -3.6436e-01, -1.3419e-01,  7.2606e-02],\n",
      "        [-7.7454e-02,  3.3946e-02,  1.3915e-01,  7.5399e-02, -5.9488e-02,\n",
      "         -2.2631e-01, -5.5670e-02,  7.1481e-02],\n",
      "        [-1.4238e-01,  5.5838e-02,  1.2818e-01,  1.4048e-01, -5.5618e-02,\n",
      "         -3.8508e-01, -1.4812e-01,  7.0334e-02],\n",
      "        [-5.9196e-02,  3.4045e-02,  1.3387e-01,  7.1816e-02, -4.9465e-02,\n",
      "         -2.1002e-01, -5.1680e-02,  6.0796e-02],\n",
      "        [-6.1111e-02,  2.6123e-02,  1.3720e-01,  6.2079e-02, -5.3327e-02,\n",
      "         -1.9751e-01, -4.1556e-02,  6.9188e-02],\n",
      "        [-2.0855e-02,  1.9857e-02,  1.3602e-01,  4.2314e-02, -4.3493e-02,\n",
      "         -1.1376e-01, -9.4757e-03,  5.2520e-02],\n",
      "        [-6.6410e-02,  2.8935e-02,  1.3110e-01,  7.1163e-02, -4.7242e-02,\n",
      "         -2.3864e-01, -6.1310e-02,  7.0179e-02],\n",
      "        [ 7.3500e-02, -8.3270e-03,  1.2660e-01,  2.8152e-03, -1.5302e-02,\n",
      "          5.9727e-02,  3.8781e-02,  5.6388e-03],\n",
      "        [-2.3479e-02,  2.2425e-02,  1.3794e-01,  3.9137e-02, -4.6632e-02,\n",
      "         -8.8581e-02,  3.4502e-04,  5.7917e-02],\n",
      "        [-1.3036e-01,  5.9936e-02,  1.1776e-01,  1.2373e-01, -4.2248e-02,\n",
      "         -3.4385e-01, -1.2032e-01,  6.2152e-02],\n",
      "        [-4.9920e-03,  1.8409e-02,  1.3617e-01,  3.9553e-02, -3.9790e-02,\n",
      "         -6.7437e-02,  6.9915e-04,  3.9880e-02],\n",
      "        [-1.1579e-01,  5.5452e-02,  1.2593e-01,  1.0058e-01, -5.0372e-02,\n",
      "         -2.7250e-01, -8.1705e-02,  5.7280e-02],\n",
      "        [-1.0651e-01,  5.5292e-02,  1.2375e-01,  1.0021e-01, -5.5523e-02,\n",
      "         -2.8667e-01, -8.7743e-02,  6.7141e-02],\n",
      "        [-2.8206e-03,  5.6600e-03,  1.3577e-01,  3.1021e-02, -4.0102e-02,\n",
      "         -9.8417e-02, -2.8404e-03,  5.4413e-02],\n",
      "        [-5.0629e-02,  3.6159e-02,  1.3458e-01,  6.3808e-02, -4.6705e-02,\n",
      "         -1.6330e-01, -3.3060e-02,  5.3340e-02],\n",
      "        [-1.4347e-03,  2.0861e-02,  1.3308e-01,  3.3035e-02, -3.8580e-02,\n",
      "         -6.5221e-02,  6.9797e-03,  4.8702e-02],\n",
      "        [-3.0749e-02,  2.3033e-02,  1.3746e-01,  5.4081e-02, -5.0082e-02,\n",
      "         -1.5931e-01, -2.7895e-02,  6.0165e-02],\n",
      "        [-5.5527e-02,  4.2528e-02,  1.2728e-01,  6.3680e-02, -4.5982e-02,\n",
      "         -1.7238e-01, -3.3958e-02,  5.4904e-02],\n",
      "        [ 2.6120e-02,  8.0679e-03,  1.3420e-01,  2.2538e-02, -3.6697e-02,\n",
      "         -3.4869e-02,  1.3837e-02,  4.7946e-02],\n",
      "        [-2.8783e-02,  2.1308e-02,  1.3768e-01,  4.5937e-02, -4.5864e-02,\n",
      "         -1.2267e-01, -1.3706e-02,  5.7379e-02],\n",
      "        [-1.7735e-02,  3.4470e-02,  1.3068e-01,  4.5136e-02, -3.8774e-02,\n",
      "         -8.8479e-02, -6.1166e-03,  4.3546e-02],\n",
      "        [ 4.6923e-02, -9.2964e-04,  1.3813e-01,  1.0618e-02, -3.3980e-02,\n",
      "          3.4926e-02,  3.8877e-02,  3.6049e-02],\n",
      "        [-3.3688e-02,  2.9185e-02,  1.3301e-01,  6.1642e-02, -5.2684e-02,\n",
      "         -1.7529e-01, -3.7215e-02,  6.2970e-02],\n",
      "        [-1.5971e-01,  6.5365e-02,  1.1763e-01,  1.4717e-01, -5.1425e-02,\n",
      "         -3.9622e-01, -1.5411e-01,  6.9249e-02],\n",
      "        [-1.1830e-01,  5.9051e-02,  1.2071e-01,  1.2296e-01, -5.0385e-02,\n",
      "         -3.3891e-01, -1.1917e-01,  6.6153e-02],\n",
      "        [ 2.1568e-02,  6.8036e-03,  1.3571e-01,  2.4177e-02, -3.2837e-02,\n",
      "         -2.5359e-02,  1.5360e-02,  3.5573e-02],\n",
      "        [-2.9197e-02,  1.8851e-02,  1.3442e-01,  5.0342e-02, -4.3754e-02,\n",
      "         -1.5365e-01, -2.6371e-02,  5.6020e-02],\n",
      "        [-9.3429e-02,  3.8659e-02,  1.3495e-01,  8.7620e-02, -5.9122e-02,\n",
      "         -2.6952e-01, -7.6611e-02,  6.8880e-02],\n",
      "        [-5.6231e-02,  3.4831e-02,  1.2872e-01,  7.1412e-02, -4.1703e-02,\n",
      "         -2.1152e-01, -5.2083e-02,  5.8163e-02],\n",
      "        [-1.1446e-01,  4.6750e-02,  1.2184e-01,  1.1024e-01, -4.5436e-02,\n",
      "         -3.4192e-01, -1.1542e-01,  7.3512e-02],\n",
      "        [ 2.4076e-02,  1.4205e-02,  1.3275e-01,  2.4857e-02, -3.0757e-02,\n",
      "         -2.9137e-02,  1.4234e-02,  3.6726e-02],\n",
      "        [-1.0567e-01,  5.3017e-02,  1.2866e-01,  8.6833e-02, -5.5066e-02,\n",
      "         -2.4112e-01, -6.3421e-02,  6.1989e-02],\n",
      "        [-1.1597e-01,  5.7674e-02,  1.1707e-01,  1.0685e-01, -4.3352e-02,\n",
      "         -3.0944e-01, -9.9209e-02,  6.6589e-02],\n",
      "        [ 5.0859e-03,  1.1364e-02,  1.3615e-01,  3.3336e-02, -3.7716e-02,\n",
      "         -8.1768e-02, -1.7720e-03,  5.1691e-02],\n",
      "        [-5.2745e-02,  3.3380e-02,  1.3078e-01,  6.7398e-02, -4.5110e-02,\n",
      "         -1.8791e-01, -4.3919e-02,  5.6211e-02],\n",
      "        [-1.6541e-01,  7.0840e-02,  1.1369e-01,  1.5716e-01, -4.3693e-02,\n",
      "         -4.0476e-01, -1.6040e-01,  6.7745e-02],\n",
      "        [-2.7914e-02,  1.9843e-02,  1.4224e-01,  4.4118e-02, -5.0095e-02,\n",
      "         -1.0408e-01, -4.7282e-03,  5.1110e-02],\n",
      "        [-6.1904e-02,  2.8268e-02,  1.3367e-01,  6.0013e-02, -5.2752e-02,\n",
      "         -2.0227e-01, -4.1737e-02,  6.8867e-02],\n",
      "        [-6.2564e-02,  2.5339e-02,  1.4409e-01,  6.8810e-02, -5.8583e-02,\n",
      "         -2.0176e-01, -4.5036e-02,  6.3088e-02]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 1  # Each RNN cell takes 1 bit as input\n",
    "hidden_size = 8 # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output\n",
    "grid_height = 4  # 4x2 grid of RNN cells\n",
    "grid_width= 2\n",
    "batch_size=64\n",
    "layers_sizes=[hidden_size*3,hidden_size*2,hidden_size ]\n",
    "\n",
    "model = LatticeRNN(input_size, hidden_size, output_size, grid_height, grid_width, layers_sizes,batch_size)\n",
    "\n",
    "# Input is a batch of binary sequences (batch_size, seq_len, input_size), reshaped to match grid size\n",
    "seq_len = grid_height * grid_width  # Number of bits equal to the grid size squared\n",
    "x = torch.randn(batch_size, seq_len, input_size)  # Random input, replace with binary input\n",
    "hidden_ext = torch.randn(batch_size, hidden_size)  # Random input, replace with binary input\n",
    "hidden_ext1 = torch.randn(batch_size, hidden_size)  # Random input, replace with binary input\n",
    "grid = [[(hidden_ext,hidden_ext1) for _ in range(grid_width)] for _ in range(grid_height)]\n",
    "print(grid[0][0])\n",
    "\n",
    "output, hidden, hidden1,grid = model(x,hidden_ext,hidden_ext1,grid)\n",
    "print(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5970\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(detection_array1, observable_flips, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m test(model, X_test, y_test,batch_size)\n",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[1;34m(model, X_train, y_train, criterion, optimizer, num_epochs, batch_size, rounds)\u001b[0m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), batch_y)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Backward pass (compute gradients)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Optimize (update weights)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\conof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\conof\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 1  # Each Lattice RNN cell takes 1 bit as input\n",
    "hidden_size = 64 # Hidden size of each RNN cell\n",
    "output_size = 1  # Binary output (e.g., 0 or 1)\n",
    "grid_height = 4  # Number of rows in the grid\n",
    "grid_width = 2   # Number of columns in the grid\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "layers_sizes=[hidden_size*3,hidden_size*2,hidden_size ]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a model instance\n",
    "model = BlockRNN(input_size, hidden_size, output_size, grid_height, grid_width, rounds,layers_sizes,batch_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "test_size=0.2\n",
    "test_dataset_size=num_shots*test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(detection_array1, observable_flips, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "train_rnn(model, X_train, y_train, criterion, optimizer, num_epochs,batch_size,rounds)\n",
    "\n",
    "test(model, X_test, y_test,batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
